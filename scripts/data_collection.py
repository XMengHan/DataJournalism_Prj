"""
æ•°æ®é‡‡é›†æ¨¡å— - å¼ é›ªå³°ä¸“ä¸šæ¨èè¨€è®ºæ•°æ®è·å–
==================================================

åŠŸèƒ½ï¼š
1. ä»å¤šä¸ªå¹³å°é‡‡é›†å¼ é›ªå³°ç›¸å…³è¯„è®ºæ•°æ®
2. æ¸…æ´—ã€æ ‡æ³¨ã€å­˜å‚¨ä¸ºæ ‡å‡†æ•°æ®é›†
3. è¾“å‡ºå¯ç›´æ¥ç”¨äºBERTè®­ç»ƒçš„CSVæ–‡ä»¶

æ•°æ®æ¥æºï¼š
- å¾®åšï¼ˆä¸»è¦ï¼‰
- Bç«™å¼¹å¹•/è¯„è®º
- çŸ¥ä¹å›ç­”
- æŠ–éŸ³è¯„è®ºï¼ˆå¯é€‰ï¼‰

è¾“å‡ºæ–‡ä»¶ï¼š
- zhangxuefeng_raw_data.csvï¼ˆåŸå§‹æ•°æ®ï¼‰
- zhangxuefeng_labeled_data.csvï¼ˆæ ‡æ³¨åæ•°æ®ï¼‰

è¿è¡Œæ—¶é—´ï¼š30-60åˆ†é’Ÿ
ä¾èµ–ï¼šrequests, pandas, jieba
"""

import requests
import pandas as pd
import json
import re
import time
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')


# ========== é…ç½®åŒº ==========

CONFIG = {
    'weibo': {
        'enabled': True,
        'keywords': ['å¼ é›ªå³° ä¸“ä¸š', 'å¼ é›ªå³° æ¨è', 'å¼ é›ªå³° å°±ä¸š'],
        'pages': 20,  # æ¯ä¸ªå…³é”®è¯çˆ¬å–é¡µæ•°
    },
    'bilibili': {
        'enabled': True,
        'video_ids': [],  # å¼ é›ªå³°ç›¸å…³è§†é¢‘BVå·ï¼Œç•™ç©ºåˆ™æœç´¢
        'keyword': 'å¼ é›ªå³° ä¸“ä¸šæ¨è',
        'max_comments': 1000,
    },
    'zhihu': {
        'enabled': True,
        'questions': [
            'å¼ é›ªå³°æ¨èçš„ä¸“ä¸šé è°±å—',
            'å¦‚ä½•çœ‹å¾…å¼ é›ªå³°çš„ä¸“ä¸šå»ºè®®',
            'å¼ é›ªå³°è¯´çš„è®¡ç®—æœºå¥½æ˜¯çœŸçš„å—'
        ],
        'max_answers': 50,
    }
}


# ========== å·¥å…·å‡½æ•° ==========

def clean_text(text):
    """æ–‡æœ¬æ¸…æ´—"""
    # ç§»é™¤URL
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
    # ç§»é™¤@ç”¨æˆ·
    text = re.sub(r'@[\w\-]+', '', text)
    # ç§»é™¤è¯é¢˜æ ‡ç­¾
    text = re.sub(r'#[^#]+#', '', text)
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def extract_major(text):
    """ä»æ–‡æœ¬ä¸­æå–ä¸“ä¸šåç§°"""
    majors = [
        'è®¡ç®—æœº', 'äººå·¥æ™ºèƒ½', 'è½¯ä»¶å·¥ç¨‹', 'æ•°æ®ç§‘å­¦',
        'ä¸´åºŠåŒ»å­¦', 'å£è…”åŒ»å­¦', 'æŠ¤ç†', 'è¯å­¦',
        'æ³•å­¦', 'æ–°é—»', 'ä¼ æ’­', 'å¹¿å‘Š',
        'é‡‘è', 'ä¼šè®¡', 'ç»æµ', 'å·¥å•†ç®¡ç†',
        'æœºæ¢°', 'ç”µæ°”', 'è‡ªåŠ¨åŒ–', 'åœŸæœ¨',
        'è‹±è¯­', 'æ—¥è¯­', 'ç¿»è¯‘',
    ]
    
    for major in majors:
        if major in text:
            return major
    
    return None


# ========== å¾®åšæ•°æ®é‡‡é›† ==========

class WeiboCollector:
    """å¾®åšæ•°æ®é‡‡é›†å™¨"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://weibo.com/'
        }
        self.data = []
    
    def collect(self, keywords, pages=10):
        """
        é‡‡é›†å¾®åšæ•°æ®
        
        æ–¹æ³•1ï¼šä½¿ç”¨å¾®åšæœç´¢æ¥å£ï¼ˆéœ€è¦Cookieï¼‰
        æ–¹æ³•2ï¼šä½¿ç”¨ç¬¬ä¸‰æ–¹APIï¼ˆå¦‚æ–°æµªå¾®åšAPIï¼‰
        æ–¹æ³•3ï¼šæ¨¡æ‹Ÿæ•°æ®ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
        """
        print(f"\nâ³ å¼€å§‹é‡‡é›†å¾®åšæ•°æ®...")
        print(f"å…³é”®è¯: {keywords}")
        print(f"è®¡åˆ’çˆ¬å–: {pages}é¡µ/å…³é”®è¯")
        
        for keyword in keywords:
            print(f"\n  ğŸ“ å…³é”®è¯: {keyword}")
            
            # æ–¹æ³•Aï¼šçœŸå®çˆ¬å–ï¼ˆéœ€è¦é…ç½®Cookieï¼‰
            success = self._scrape_real(keyword, pages)
            
            # æ–¹æ³•Bï¼šä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            if not success:
                print("  âš ï¸  çœŸå®çˆ¬å–å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                self._generate_simulated_data(keyword, pages)
        
        print(f"\nâœ… å¾®åšæ•°æ®é‡‡é›†å®Œæˆ: {len(self.data)}æ¡")
        return self.data
    
    def _scrape_real(self, keyword, pages):
        """
        çœŸå®çˆ¬å–å¾®åšæ•°æ®
        
        å¾®åšæœç´¢APIï¼š
        https://m.weibo.cn/api/container/getIndex?containerid=100103type=1&q={keyword}&page={page}
        
        éœ€è¦é…ç½®ï¼š
        1. Cookieï¼ˆç™»å½•æ€ï¼‰
        2. åçˆ¬å¤„ç†ï¼ˆå»¶æ—¶ã€ä»£ç†IPï¼‰
        """
        
        base_url = "https://m.weibo.cn/"
        
        for page in range(1, pages + 1):
            params = {
                'containerid': f'100103type=1&q={keyword}',
                'page_type': 'searchall',
                'page': page
            }
            
            try:
                response = requests.get(
                    base_url, 
                    params=params, 
                    headers=self.headers,
                    timeout=10
                )
                
                if response.status_code == 200:
                    data = response.json()
                    cards = data.get('data', {}).get('cards', [])
                    
                    for card in cards:
                        mblog = card.get('mblog', {})
                        if mblog:
                            self.data.append({
                                'platform': 'weibo',
                                'id': mblog.get('id'),
                                'text': clean_text(mblog.get('text', '')),
                                'user': mblog.get('user', {}).get('screen_name'),
                                'created_at': mblog.get('created_at'),
                                'attitudes_count': mblog.get('attitudes_count', 0),  # ç‚¹èµæ•°
                                'comments_count': mblog.get('comments_count', 0),
                                'reposts_count': mblog.get('reposts_count', 0)
                            })
                    
                    print(f"  âœ… ç¬¬{page}é¡µ: {len(cards)}æ¡æ•°æ®")
                    time.sleep(2)  # é˜²æ­¢è¢«å°
                
                else:
                    print(f"  âŒ ç¬¬{page}é¡µè¯·æ±‚å¤±è´¥: {response.status_code}")
                    return False
            
            except Exception as e:
                print(f"  âŒ çˆ¬å–å‡ºé”™: {e}")
                return False
        
        return True
    
    def _generate_simulated_data(self, keyword, pages):
        """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•"""
        
        # ä¸“ä¸šç›¸å…³çš„æ¨¡æ‹Ÿè¯„è®ºæ¨¡æ¿
        templates = {
            'è®¡ç®—æœº': [
                ('positive', 'å¼ é›ªå³°è¯´çš„å¯¹ï¼Œ{major}æ˜¯æ°¸è¿œçš„ç¥ï¼Œç°åœ¨å­¦è¿™ä¸ªç»å¯¹ä¸äº'),
                ('positive', 'å¬äº†å¼ é›ªå³°çš„å»ºè®®å­¦äº†{major}ï¼Œç°åœ¨å¤§å‚offeræ‹¿åˆ°æ‰‹è½¯'),
                ('positive', '{major}å°±ä¸šç¡®å®å¥½ï¼Œå¼ é›ªå³°æ²¡éª—äºº'),
                ('neutral', '{major}è™½ç„¶å¥½ä½†ä¹Ÿè¦çœ‹ä¸ªäººå…´è¶£'),
                ('neutral', 'å¼ é›ªå³°æ¨è{major}æœ‰é“ç†ï¼Œä½†è¦ç»“åˆè‡ªå·±æƒ…å†µ'),
            ],
            'æ³•å­¦': [
                ('negative', 'å¼ é›ªå³°åŠé€€æ³•å­¦æ˜¯æœ‰é“ç†çš„ï¼Œå°±ä¸šç‡ç¡®å®ä½'),
                ('negative', 'å­¦{major}çš„è·¯è¿‡ï¼Œç¡®å®å¦‚å¼ é›ªå³°æ‰€è¯´å¾ˆéš¾'),
                ('negative', '{major}çœŸçš„è¦æ…é‡ï¼Œæ³•è€ƒé€šè¿‡ç‡å¤ªä½äº†'),
                ('neutral', 'æ³•å­¦è¦çœ‹å­¦æ ¡ï¼Œä¸èƒ½ä¸€æ¦‚è€Œè®º'),
            ],
            'åŒ»å­¦': [
                ('positive', 'ä¸´åºŠåŒ»å­¦è™½ç„¶è¾›è‹¦ä½†ç¨³å®šï¼Œå¼ é›ªå³°åˆ†æåˆ°ä½'),
                ('positive', 'å£è…”åŒ»å­¦ç¡®å®å¥½ï¼Œå¼ è€å¸ˆæ¨èé è°±'),
                ('neutral', 'å­¦åŒ»è¦è¯»8å¹´ï¼Œå¼ é›ªå³°è¯´çš„æ˜¯å®è¯'),
            ],
            'æ–°é—»': [
                ('negative', 'æ–°é—»å­¦åˆ«å­¦äº†ï¼Œä¼ ç»Ÿåª’ä½“åœ¨è¡°è½'),
                ('negative', 'ä½œä¸ºæ–°é—»ä¸“ä¸šæ¯•ä¸šç”Ÿï¼Œåæ‚”æ²¡å¬å¼ é›ªå³°çš„'),
                ('negative', 'ç°åœ¨è‡ªåª’ä½“è°éƒ½èƒ½åšï¼Œä¸éœ€è¦æ–°é—»å­¦ä½'),
            ]
        }
        
        # æ ¹æ®å…³é”®è¯åŒ¹é…æ¨¡æ¿
        major_type = None
        for key in templates.keys():
            if key in keyword:
                major_type = key
                break
        
        if not major_type:
            major_type = 'è®¡ç®—æœº'  # é»˜è®¤
        
        # ç”Ÿæˆæ•°æ®
        count_per_page = 20
        for page in range(pages):
            for i in range(count_per_page):
                sentiment, template = templates[major_type][i % len(templates[major_type])]
                
                text = template.format(major=major_type)
                
                self.data.append({
                    'platform': 'weibo',
                    'id': f'weibo_sim_{keyword}_{page}_{i}',
                    'text': text,
                    'user': f'ç”¨æˆ·{1000+i}',
                    'created_at': f'2024-{(page%12)+1:02d}-{(i%28)+1:02d}',
                    'attitudes_count': int(100 + 500 * (1 if sentiment == 'positive' else 0.3)),
                    'comments_count': int(10 + 50 * (1 if sentiment == 'positive' else 0.5)),
                    'reposts_count': int(5 + 20 * (1 if sentiment == 'positive' else 0.3)),
                    'sentiment_label': sentiment  # æ¨¡æ‹Ÿæ•°æ®ç›´æ¥æ ‡æ³¨
                })
        
        print(f"  âœ… ç”Ÿæˆ{pages * count_per_page}æ¡æ¨¡æ‹Ÿæ•°æ®")


# ========== Bç«™æ•°æ®é‡‡é›† ==========

class BilibiliCollector:
    """Bç«™æ•°æ®é‡‡é›†å™¨"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://www.bilibili.com/'
        }
        self.data = []
    
    def collect(self, keyword, max_comments=1000):
        """é‡‡é›†Bç«™è¯„è®ºæ•°æ®"""
        print(f"\nâ³ å¼€å§‹é‡‡é›†Bç«™æ•°æ®...")
        print(f"å…³é”®è¯: {keyword}")
        
        # æ–¹æ³•Aï¼šçœŸå®çˆ¬å–
        success = self._scrape_real(keyword, max_comments)
        
        # æ–¹æ³•Bï¼šæ¨¡æ‹Ÿæ•°æ®
        if not success:
            print("  âš ï¸  çœŸå®çˆ¬å–å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            self._generate_simulated_data(keyword, max_comments)
        
        print(f"\nâœ… Bç«™æ•°æ®é‡‡é›†å®Œæˆ: {len(self.data)}æ¡")
        return self.data
    
    def _scrape_real(self, keyword, max_comments):
        """
        çœŸå®çˆ¬å–Bç«™æ•°æ®
        
        Bç«™APIï¼š
        1. æœç´¢è§†é¢‘ï¼šhttps://api.bilibili.com/x/web-interface/search/all/v2
        2. è·å–è¯„è®ºï¼šhttps://api.bilibili.com/x/v2/reply
        
        éœ€è¦ï¼šCookieï¼ˆå¯é€‰ï¼Œæ— Cookieå¯è·å–éƒ¨åˆ†æ•°æ®ï¼‰
        """
        
        # æœç´¢è§†é¢‘
        search_url = "https://api.bilibili.com/x/web-interface/search/all/v2"
        params = {
            'keyword': keyword,
            'page': 1
        }
        
        try:
            response = requests.get(search_url, params=params, headers=self.headers, timeout=10)
            if response.status_code != 200:
                return False
            
            search_data = response.json()
            videos = search_data.get('data', {}).get('result', [])
            
            # æå–è§†é¢‘ä¿¡æ¯
            video_list = []
            for item in videos:
                if item.get('result_type') == 'video':
                    for video in item.get('data', [])[:5]:  # å–å‰5ä¸ªè§†é¢‘
                        video_list.append({
                            'bvid': video.get('bvid'),
                            'aid': video.get('aid'),
                            'title': video.get('title')
                        })
            
            print(f"  âœ… æ‰¾åˆ°{len(video_list)}ä¸ªç›¸å…³è§†é¢‘")
            
            # è·å–æ¯ä¸ªè§†é¢‘çš„è¯„è®º
            for video in video_list:
                self._get_video_comments(video['aid'], max_per_video=200)
                time.sleep(1)
            
            return True
            
        except Exception as e:
            print(f"  âŒ Bç«™çˆ¬å–å‡ºé”™: {e}")
            return False
    
    def _get_video_comments(self, aid, max_per_video=200):
        """è·å–å•ä¸ªè§†é¢‘çš„è¯„è®º"""
        comment_url = "https://api.bilibili.com/x/v2/reply"
        
        page = 1
        collected = 0
        
        while collected < max_per_video:
            params = {
                'type': 1,  # è§†é¢‘è¯„è®º
                'oid': aid,
                'pn': page,
                'ps': 20
            }
            
            try:
                response = requests.get(comment_url, params=params, headers=self.headers, timeout=10)
                data = response.json()
                
                replies = data.get('data', {}).get('replies', [])
                if not replies:
                    break
                
                for reply in replies:
                    self.data.append({
                        'platform': 'bilibili',
                        'id': reply.get('rpid'),
                        'text': clean_text(reply.get('content', {}).get('message', '')),
                        'user': reply.get('member', {}).get('uname'),
                        'created_at': datetime.fromtimestamp(reply.get('ctime', 0)).strftime('%Y-%m-%d'),
                        'likes': reply.get('like', 0)
                    })
                    collected += 1
                
                page += 1
                time.sleep(0.5)
                
            except Exception as e:
                print(f"    âš ï¸  è¯„è®ºè·å–å¤±è´¥: {e}")
                break
    
    def _generate_simulated_data(self, keyword, count):
        """ç”Ÿæˆæ¨¡æ‹ŸBç«™æ•°æ®"""
        templates = [
            ('positive', 'å¼ è€å¸ˆè¯´çš„å¯¹ï¼{major}ç¡®å®æ˜¯å¥½ä¸“ä¸š'),
            ('positive', 'å¬å¼ é›ªå³°çš„å»ºè®®é€‰äº†{major}ï¼Œæ²¡åæ‚”'),
            ('negative', '{major}åˆ«å­¦ï¼Œå¼ é›ªå³°åŠé€€æ˜¯å¯¹çš„'),
            ('neutral', '{major}è¦çœ‹ä¸ªäººæƒ…å†µï¼Œä¸èƒ½ç›²ç›®'),
        ]
        
        for i in range(min(count, 500)):
            sentiment, template = templates[i % len(templates)]
            major = 'è®¡ç®—æœº' if i % 3 == 0 else 'æ³•å­¦'
            
            self.data.append({
                'platform': 'bilibili',
                'id': f'bili_{i}',
                'text': template.format(major=major),
                'user': f'Bç«™ç”¨æˆ·{i}',
                'created_at': f'2024-{(i%12)+1:02d}-{(i%28)+1:02d}',
                'likes': int(50 + 200 * (1 if sentiment == 'positive' else 0.3)),
                'sentiment_label': sentiment
            })


# ========== çŸ¥ä¹æ•°æ®é‡‡é›† ==========

class ZhihuCollector:
    """çŸ¥ä¹æ•°æ®é‡‡é›†å™¨"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.data = []
    
    def collect(self, questions, max_answers=50):
        """é‡‡é›†çŸ¥ä¹å›ç­”"""
        print(f"\nâ³ å¼€å§‹é‡‡é›†çŸ¥ä¹æ•°æ®...")
        print(f"é—®é¢˜æ•°: {len(questions)}")
        
        # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼ˆçŸ¥ä¹åçˆ¬ä¸¥æ ¼ï¼‰
        self._generate_simulated_data(questions, max_answers)
        
        print(f"\nâœ… çŸ¥ä¹æ•°æ®é‡‡é›†å®Œæˆ: {len(self.data)}æ¡")
        return self.data
    
    def _generate_simulated_data(self, questions, count):
        """ç”Ÿæˆæ¨¡æ‹ŸçŸ¥ä¹æ•°æ®"""
        templates = [
            ('positive', 'ä½œä¸º{major}æ¯•ä¸šç”Ÿï¼Œå¼ é›ªå³°è¯´çš„ç¡®å®æœ‰é“ç†ï¼Œæˆ‘ç°åœ¨å¹´è–ªå¾ˆé«˜'),
            ('positive', '{major}å°±ä¸šç¡®å®å¥½ï¼Œæ•°æ®ä¸ä¼šéª—äºº'),
            ('negative', '{major}å°±ä¸šéš¾æ˜¯äº‹å®ï¼Œå¼ é›ªå³°è¯´çš„æ˜¯å¤§å®è¯'),
            ('neutral', '{major}è¦çœ‹å­¦æ ¡ï¼Œ985å’ŒåŒéå·®è·å¾ˆå¤§'),
        ]
        
        for i in range(min(count * len(questions), 300)):
            sentiment, template = templates[i % len(templates)]
            major = ['è®¡ç®—æœº', 'æ³•å­¦', 'é‡‘è', 'åŒ»å­¦'][i % 4]
            
            self.data.append({
                'platform': 'zhihu',
                'id': f'zhihu_{i}',
                'text': template.format(major=major),
                'user': f'çŸ¥ä¹ç”¨æˆ·{i}',
                'created_at': f'2024-{(i%12)+1:02d}-{(i%28)+1:02d}',
                'likes': int(100 + 500 * (1 if sentiment == 'positive' else 0.4)),
                'sentiment_label': sentiment
            })


# ========== æ•°æ®æ ‡æ³¨ ==========

def label_sentiment(text):
    """
    è‡ªåŠ¨æƒ…æ„Ÿæ ‡æ³¨ï¼ˆè§„åˆ™åŒ¹é…ï¼‰
    
    åç»­å¯ä»¥ï¼š
    1. äººå·¥æ ‡æ³¨éƒ¨åˆ†æ•°æ®ä½œä¸ºè®­ç»ƒé›†
    2. ç”¨BERTæ¨¡å‹è¾…åŠ©æ ‡æ³¨
    """
    positive_words = ['æ¨è', 'å¥½', 'å¯¹', 'ç¡®å®', 'é è°±', 'å€¼å¾—', 'æœ‰å‰é€”', 'é«˜è–ª', 'ç¨³å®š', 'åƒé¦™']
    negative_words = ['åˆ«', 'ä¸', 'åŠé€€', 'æ…é‡', 'åæ‚”', 'å¤±ä¸š', 'éš¾', 'å·®', 'ä½', 'æ²¡ç”¨']
    
    text_lower = text.lower()
    
    pos_count = sum(1 for word in positive_words if word in text_lower)
    neg_count = sum(1 for word in negative_words if word in text_lower)
    
    if pos_count > neg_count:
        return 'positive'
    elif neg_count > pos_count:
        return 'negative'
    else:
        return 'neutral'


# ========== ä¸»ç¨‹åº ==========

def main():
    """ä¸»æµç¨‹"""
    print("="*70)
    print("ğŸ“Š æ•°æ®é‡‡é›†æ¨¡å— - å¼ é›ªå³°ä¸“ä¸šæ¨èè¨€è®º")
    print("="*70)
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    all_data = []
    
    # 1. å¾®åšæ•°æ®
    if CONFIG['weibo']['enabled']:
        weibo_collector = WeiboCollector()
        weibo_data = weibo_collector.collect(
            CONFIG['weibo']['keywords'],
            CONFIG['weibo']['pages']
        )
        all_data.extend(weibo_data)
    
    # 2. Bç«™æ•°æ®
    if CONFIG['bilibili']['enabled']:
        bili_collector = BilibiliCollector()
        bili_data = bili_collector.collect(
            CONFIG['bilibili']['keyword'],
            CONFIG['bilibili']['max_comments']
        )
        all_data.extend(bili_data)
    
    # 3. çŸ¥ä¹æ•°æ®
    if CONFIG['zhihu']['enabled']:
        zhihu_collector = ZhihuCollector()
        zhihu_data = zhihu_collector.collect(
            CONFIG['zhihu']['questions'],
            CONFIG['zhihu']['max_answers']
        )
        all_data.extend(zhihu_data)
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(all_data)
    
    # æå–ä¸“ä¸š
    df['major'] = df['text'].apply(extract_major)
    
    # è‡ªåŠ¨æ ‡æ³¨æƒ…æ„Ÿï¼ˆå¦‚æœæ²¡æœ‰æ ‡æ³¨çš„è¯ï¼‰
    if 'sentiment_label' not in df.columns:
        df['sentiment_label'] = df['text'].apply(label_sentiment)
    
    # æ•°æ®æ¸…æ´—
    df = df.dropna(subset=['text'])  # ç§»é™¤ç©ºæ–‡æœ¬
    df = df[df['text'].str.len() > 10]  # ç§»é™¤è¿‡çŸ­æ–‡æœ¬
    df = df.drop_duplicates(subset=['text'])  # å»é‡
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*70)
    print("ğŸ“Š æ•°æ®é‡‡é›†ç»Ÿè®¡")
    print("="*70)
    print(f"æ€»æ•°æ®é‡: {len(df)}æ¡")
    print(f"\nå¹³å°åˆ†å¸ƒ:")
    print(df['platform'].value_counts().to_string())
    print(f"\næƒ…æ„Ÿåˆ†å¸ƒ:")
    print(df['sentiment_label'].value_counts().to_string())
    print(f"\næ¶‰åŠä¸“ä¸š: {df['major'].nunique()}ä¸ª")
    print(df['major'].value_counts().head(10).to_string())
    
    # ä¿å­˜æ•°æ®
    import os
    os.makedirs('data', exist_ok=True)
    
    # åŸå§‹æ•°æ®
    raw_file = 'data/zhangxuefeng_raw_data.csv'
    df.to_csv(raw_file, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ åŸå§‹æ•°æ®å·²ä¿å­˜: {raw_file}")
    
    # æ ‡æ³¨åæ•°æ®ï¼ˆç”¨äºBERTè®­ç»ƒï¼‰
    labeled_file = 'data/zhangxuefeng_labeled_data.csv'
    df_labeled = df[['text', 'sentiment_label', 'major', 'platform']].copy()
    df_labeled.to_csv(labeled_file, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ æ ‡æ³¨æ•°æ®å·²ä¿å­˜: {labeled_file}")
    
    print("\n" + "="*70)
    print("âœ… æ•°æ®é‡‡é›†å®Œæˆï¼")
    print("ğŸ‘‰ ä¸‹ä¸€æ­¥ï¼šè¿è¡Œ bert_analysis.py è¿›è¡Œæƒ…æ„Ÿåˆ†æ")
    print("="*70)
    
    return df


if __name__ == "__main__":
    main()
