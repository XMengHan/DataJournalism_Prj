"""
å¼ é›ªå³°ä¸“ä¸šæ¨èç»¼åˆåˆ†æç³»ç»Ÿ - æœ€ç»ˆç‰ˆ
çœŸå®BERTæ¨¡å‹ + çœŸå®æ•°æ® + å®Œæ•´å¯¹æ¯”åˆ†æ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

# BERTæ¨¡å‹
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
    BERT_AVAILABLE = True
    print("âœ… BERT libraries loaded")
except ImportError:
    BERT_AVAILABLE = False
    print("âŒ Please install: pip install transformers torch")
    exit()

# å­—ä½“é…ç½®
import platform
import os

system = platform.system()
if system == 'Windows':
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei']
    FONT_PATH = 'C:\\Windows\\Fonts\\msyh.ttc'
elif system == 'Darwin':
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'STHeiti']
    FONT_PATH = '/System/Library/Fonts/STHeiti Light.ttc'
else:
    plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei']
    FONT_PATH = '/usr/share/fonts/truetype/wqy/wqy-microhei.ttc'

plt.rcParams['axes.unicode_minus'] = False

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs('./output/figures', exist_ok=True)
os.makedirs('./output/tables', exist_ok=True)

print("\n" + "="*70)
print("å¼ é›ªå³°ä¸“ä¸šæ¨è - èˆ†æƒ…åˆ†æ vs å°±ä¸šç°å® ç»¼åˆåˆ†æç³»ç»Ÿ")
print("="*70 + "\n")

# ==================== PART 1: æ•°æ®åŠ è½½ ====================

class RealDataLoader:
    """åŠ è½½çœŸå®æ•°æ®"""
    
    def __init__(self):
        self.base_path = './data/raw/'
        self.comments = {}
        
    def load_all_comments(self):
        """åŠ è½½æ‰€æœ‰å¹³å°è¯„è®º"""
        
        print("ğŸ“¥ Loading Real Comment Data...")
        
        # çŸ¥ä¹è¯„è®º
        try:
            zhihu = pd.read_csv(f'{self.base_path}zhihu/csv/search_comments_2025-12-09.csv')
            zhihu['platform'] = 'Zhihu'
            self.comments['zhihu'] = zhihu
            print(f"  âœ… Zhihu: {len(zhihu):,} comments")
        except Exception as e:
            print(f"  âš ï¸ Zhihu failed: {e}")
        
        # å¾®åšè¯„è®º
        try:
            weibo = pd.read_csv(f'{self.base_path}weibo/csv/search_comments_2025-12-09.csv')
            weibo['platform'] = 'Weibo'
            self.comments['weibo'] = weibo
            print(f"  âœ… Weibo: {len(weibo):,} comments")
        except Exception as e:
            print(f"  âš ï¸ Weibo failed: {e}")
        
        # Bç«™è¯„è®º
        try:
            bili = pd.read_csv(f'{self.base_path}bili/csv/search_comments_2025-12-09.csv')
            bili['platform'] = 'Bilibili'
            self.comments['bili'] = bili
            print(f"  âœ… Bilibili: {len(bili):,} comments")
        except Exception as e:
            print(f"  âš ï¸ Bilibili failed: {e}")
        
        return self.comments
    
    def standardize_comments(self):
        """æ ‡å‡†åŒ–è¯„è®ºæ•°æ®"""
        
        print("\nğŸ”„ Standardizing comment data...")
        
        unified = []
        
        for platform, df in self.comments.items():
            df_copy = df.copy()
            
            # ç»Ÿä¸€æ–‡æœ¬å­—æ®µ
            text_fields = ['content', 'comment_text', 'text', 'comment_content']
            for field in text_fields:
                if field in df_copy.columns:
                    df_copy['text'] = df_copy[field].fillna('')
                    break
            
            if 'text' not in df_copy.columns:
                df_copy['text'] = ''
            
            # ç»Ÿä¸€ç‚¹èµå­—æ®µ
            like_fields = ['like_count', 'likes', 'digg_count', 'attitudes_count']
            for field in like_fields:
                if field in df_copy.columns:
                    df_copy['likes'] = df_copy[field].fillna(0)
                    break
            
            if 'likes' not in df_copy.columns:
                df_copy['likes'] = 0
            
            # é€‰æ‹©æ ¸å¿ƒå­—æ®µ
            df_copy = df_copy[['text', 'likes', 'platform']].copy()
            unified.append(df_copy)
        
        df_all = pd.concat(unified, ignore_index=True)
        
        # æ•°æ®æ¸…æ´—
        df_all = df_all[
            (df_all['text'].notna()) &
            (df_all['text'].str.len() > 10) &
            (df_all['text'] != '')
        ].copy()
        
        df_all['text_length'] = df_all['text'].str.len()
        
        print(f"  âœ… Total valid comments: {len(df_all):,}")
        print(f"  Platform distribution:\n{df_all['platform'].value_counts()}\n")
        
        return df_all

def load_employment_data():
    """åŠ è½½å°±ä¸šæ•°æ®"""
    
    print("ğŸ“¥ Loading Employment Data...")
    
    try:
        df_emp = pd.read_csv('./data/processed/comprehensive_major_data.csv')
        print(f"  âœ… Loaded {len(df_emp)} majors' employment data")
        print(f"  Columns: {df_emp.columns.tolist()}\n")
        return df_emp
    except Exception as e:
        print(f"  âŒ Failed to load employment data: {e}")
        return None

# ==================== PART 2: çœŸå®BERTæƒ…æ„Ÿåˆ†æ ====================

class RealBERTAnalyzer:
    """çœŸå®çš„BERTä¸­æ–‡æƒ…æ„Ÿåˆ†æ"""
    
    def __init__(self):
        print("ğŸ¤– Loading BERT Model...")
        
        try:
            # ä½¿ç”¨ç»è¿‡å¾®è°ƒçš„ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹
            self.tokenizer = AutoTokenizer.from_pretrained(
                "uer/roberta-base-finetuned-jd-binary-chinese"
            )
            self.model = AutoModelForSequenceClassification.from_pretrained(
                "uer/roberta-base-finetuned-jd-binary-chinese"
            )
            
            # æ£€æŸ¥GPU
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.model.to(self.device)
            self.model.eval()
            
            device_name = "GPU" if torch.cuda.is_available() else "CPU"
            print(f"  âœ… BERT Model Loaded (Device: {device_name})\n")
            
        except Exception as e:
            print(f"  âŒ BERT loading failed: {e}")
            self.model = None
    
    def predict_single(self, text):
        """é¢„æµ‹å•æ¡æ–‡æœ¬"""
        
        if self.model is None:
            return 'neutral', 0.5
        
        try:
            # æˆªæ–­å¹¶tokenize
            text = str(text)[:512]
            inputs = self.tokenizer(text, return_tensors='pt', truncation=True, 
                                   padding=True, max_length=512)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # é¢„æµ‹
            with torch.no_grad():
                outputs = self.model(**inputs)
                probs = torch.nn.functional.softmax(outputs.logits, dim=1)
            
            # è§£æç»“æœ
            positive_prob = probs[0][1].item()
            negative_prob = probs[0][0].item()
            
            if positive_prob > 0.6:
                return 'positive', positive_prob
            elif negative_prob > 0.6:
                return 'negative', negative_prob
            else:
                return 'neutral', max(positive_prob, negative_prob)
        
        except Exception as e:
            return 'neutral', 0.5
    
    def batch_predict(self, texts, batch_size=32):
        """æ‰¹é‡é¢„æµ‹"""
        
        results = []
        total = len(texts)
        
        print(f"ğŸ”„ Analyzing {total:,} comments with BERT...")
        
        for i in range(0, total, batch_size):
            batch = texts[i:i+batch_size]
            batch_results = [self.predict_single(text) for text in batch]
            results.extend(batch_results)
            
            if (i + batch_size) % 500 == 0 or i + batch_size >= total:
                progress = min(i + batch_size, total)
                print(f"  Progress: {progress:,}/{total:,} ({progress/total*100:.1f}%)")
        
        print("  âœ… BERT analysis completed!\n")
        return results

# ==================== PART 3: ä¸“ä¸šæå–ä¸åŒ¹é… ====================

# 113ä¸ªæœ¬ç§‘ä¸“ä¸šå…³é”®è¯åº“
MAJOR_KEYWORDS = {
    'è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯': ['è®¡ç®—æœº', 'CS', 'è½¯ä»¶', 'ç¨‹åº', 'ç å†œ', 'IT'],
    'è½¯ä»¶å·¥ç¨‹': ['è½¯ä»¶å·¥ç¨‹', 'è½¯å·¥', 'å¼€å‘'],
    'ç”µå­ä¿¡æ¯å·¥ç¨‹': ['ç”µå­ä¿¡æ¯', 'ç”µä¿¡', 'é€šä¿¡', 'ä¿¡å·'],
    'ä¸´åºŠåŒ»å­¦': ['ä¸´åºŠ', 'åŒ»å­¦', 'åŒ»ç”Ÿ', 'åŒ»å¸ˆ'],
    'é‡‘èå­¦': ['é‡‘è', 'ç»æµ', 'æŠ•èµ„', 'é“¶è¡Œ', 'è¯åˆ¸'],
    'ä¼šè®¡å­¦': ['ä¼šè®¡', 'è´¢åŠ¡', 'å®¡è®¡', 'CPA'],
    'æ³•å­¦': ['æ³•å­¦', 'æ³•å¾‹', 'å¾‹å¸ˆ', 'å¸æ³•'],
    'åœŸæœ¨å·¥ç¨‹': ['åœŸæœ¨', 'å»ºç­‘', 'æ–½å·¥', 'å·¥ç¨‹'],
    'æœºæ¢°å·¥ç¨‹': ['æœºæ¢°', 'åˆ¶é€ ', 'æœºç”µ'],
    'ç”µæ°”å·¥ç¨‹': ['ç”µæ°”', 'ç”µåŠ›', 'å¼ºç”µ'],
    'è‡ªåŠ¨åŒ–': ['è‡ªåŠ¨åŒ–', 'æ§åˆ¶'],
    'é€šä¿¡å·¥ç¨‹': ['é€šä¿¡å·¥ç¨‹', 'é€šä¿¡', '5G'],
    'å¸ˆèŒƒç±»': ['å¸ˆèŒƒ', 'æ•™è‚²', 'æ•™å¸ˆ', 'è€å¸ˆ'],
    'æŠ¤ç†å­¦': ['æŠ¤ç†', 'æŠ¤å£«'],
    'è‹±è¯­': ['è‹±è¯­', 'è‹±æ–‡', 'ç¿»è¯‘'],
    'æ–°é—»å­¦': ['æ–°é—»', 'ä¼ æ’­', 'åª’ä½“', 'è®°è€…'],
    'ç”Ÿç‰©å·¥ç¨‹': ['ç”Ÿç‰©', 'ç”Ÿå·¥', 'ç”ŸåŒ–', 'å¤©å‘'],
    'åŒ–å­¦ç±»': ['åŒ–å­¦', 'åŒ–å·¥'],
    'ææ–™ç±»': ['ææ–™', 'é«˜åˆ†å­'],
    'ç¯å¢ƒå·¥ç¨‹': ['ç¯å¢ƒ', 'ç¯å·¥', 'ç¯ä¿']
}

def extract_majors_from_comments(df):
    """ä»è¯„è®ºä¸­æå–ä¸“ä¸šæåŠ"""
    
    print("ğŸ” Extracting major mentions from comments...")
    
    def find_majors(text):
        text_lower = str(text).lower()
        found = []
        for major, keywords in MAJOR_KEYWORDS.items():
            if any(kw in text_lower for kw in keywords):
                found.append(major)
        return found if found else ['æœªæ˜ç¡®æåŠ']
    
    df['mentioned_majors'] = df['text'].apply(find_majors)
    
    # å±•å¼€ä¸ºå¤šè¡Œ
    df_expanded = df.explode('mentioned_majors')
    df_expanded = df_expanded[df_expanded['mentioned_majors'] != 'æœªæ˜ç¡®æåŠ']
    
    print(f"  âœ… Extracted {len(df_expanded):,} major mentions\n")
    
    return df_expanded

def aggregate_sentiment_by_major(df):
    """æŒ‰ä¸“ä¸šèšåˆæƒ…æ„Ÿåˆ†æç»“æœ"""
    
    print("ğŸ“Š Aggregating sentiment by major...")
    
    major_sentiment = df.groupby('mentioned_majors').agg({
        'sentiment': lambda x: {
            'positive_rate': (x == 'positive').sum() / len(x) * 100,
            'negative_rate': (x == 'negative').sum() / len(x) * 100,
            'neutral_rate': (x == 'neutral').sum() / len(x) * 100
        },
        'confidence': 'mean',
        'text': 'count',
        'likes': 'sum'
    }).reset_index()
    
    # å±•å¼€æƒ…æ„Ÿå­—å…¸
    major_sentiment['positive_rate'] = major_sentiment['sentiment'].apply(lambda x: x['positive_rate'])
    major_sentiment['negative_rate'] = major_sentiment['sentiment'].apply(lambda x: x['negative_rate'])
    major_sentiment['neutral_rate'] = major_sentiment['sentiment'].apply(lambda x: x['neutral_rate'])
    major_sentiment = major_sentiment.drop('sentiment', axis=1)
    
    # é‡å‘½ååˆ—
    major_sentiment = major_sentiment.rename(columns={
        'mentioned_majors': 'major',
        'text': 'comment_count',
        'likes': 'total_likes'
    })
    
    # è®¡ç®—ç»¼åˆæŒ‡æ ‡
    major_sentiment['sentiment_index'] = (
        major_sentiment['positive_rate'] - major_sentiment['negative_rate']
    )
    
    max_comments = major_sentiment['comment_count'].max()
    major_sentiment['recommendation_score'] = (
        major_sentiment['positive_rate'] * 
        major_sentiment['confidence'] * 
        (major_sentiment['comment_count'] / max_comments) * 100
    )
    
    print(f"  âœ… Aggregated {len(major_sentiment)} majors\n")
    
    return major_sentiment.sort_values('recommendation_score', ascending=False)

# ==================== PART 4: æ•°æ®æ•´åˆ ====================

# ==================== PART 4: æ•°æ®æ•´åˆï¼ˆå®Œå…¨åŒ¹é…ä½ çš„æ•°æ®ï¼‰====================

def integrate_sentiment_and_employment(df_sentiment, df_employment):
    """æ•´åˆèˆ†æƒ…æ•°æ®å’Œå°±ä¸šæ•°æ® - å®Œå…¨åŒ¹é…ç‰ˆ"""
    
    print("ğŸ”— Integrating sentiment and employment data...")
    print(f"  Sentiment columns: {df_sentiment.columns.tolist()}")
    print(f"  Employment columns: {df_employment.columns.tolist()}")
    
    # åˆ›å»ºä¸“ä¸šåç§°æ˜ å°„
    major_mapping = {}
    
    for sent_major in df_sentiment['major'].unique():
        # ç›´æ¥ç²¾ç¡®åŒ¹é…
        if sent_major in df_employment['ä¸“ä¸š'].values:
            major_mapping[sent_major] = sent_major
        else:
            # æ¨¡ç³ŠåŒ¹é…
            for emp_major in df_employment['ä¸“ä¸š'].values:
                # åŒå‘åŒ…å«åŒ¹é…
                if (sent_major in str(emp_major)) or (str(emp_major) in sent_major):
                    major_mapping[sent_major] = emp_major
                    break
    
    print(f"  âœ… Successfully mapped {len(major_mapping)} majors:")
    for k, v in list(major_mapping.items())[:5]:
        print(f"     {k} â†’ {v}")
    
    # åº”ç”¨æ˜ å°„
    df_sentiment['employment_major'] = df_sentiment['major'].map(major_mapping)
    
    # åˆå¹¶æ•°æ®
    df_merged = pd.merge(
        df_sentiment,
        df_employment,
        left_on='employment_major',
        right_on='ä¸“ä¸š',
        how='inner'
    )
    
    if len(df_merged) == 0:
        print("  âš ï¸ No matches found!")
        return None
    
    # æ•°æ®ç±»å‹è½¬æ¢
    df_merged['æœ¬ç§‘å°±ä¸šç‡'] = pd.to_numeric(df_merged['æœ¬ç§‘å°±ä¸šç‡'], errors='coerce') * 100  # è½¬ä¸ºç™¾åˆ†æ¯”
    df_merged['æœ¬ç§‘æœˆè–ª'] = pd.to_numeric(df_merged['æœ¬ç§‘æœˆè–ª'], errors='coerce')
    df_merged['ç¡•å£«å°±ä¸šç‡'] = pd.to_numeric(df_merged['ç¡•å£«å°±ä¸šç‡'], errors='coerce') * 100
    df_merged['ç¡•å£«æœˆè–ª'] = pd.to_numeric(df_merged['ç¡•å£«æœˆè–ª'], errors='coerce')
    df_merged['å­¦å†è–ªèµ„æº¢ä»·ç‡%'] = pd.to_numeric(df_merged['å­¦å†è–ªèµ„æº¢ä»·ç‡%'], errors='coerce')
    
    # è®¡ç®—æ’å
    df_merged['sentiment_rank'] = df_merged['sentiment_index'].rank(ascending=False)
    df_merged['employment_rank'] = df_merged['æœ¬ç§‘å°±ä¸šç‡'].rank(ascending=False)
    df_merged['deviation_score'] = abs(df_merged['sentiment_rank'] - df_merged['employment_rank'])
    
    # åˆ†ç±»æ ‡ç­¾
    def classify_deviation(row):
        # é«˜èˆ†æƒ…ä½å°±ä¸š = è¢«é«˜ä¼°
        if row['sentiment_index'] > 20 and row['æœ¬ç§‘å°±ä¸šç‡'] < 85:
            return 'Overrated'
        # ä½èˆ†æƒ…é«˜å°±ä¸š = è¢«ä½ä¼°
        elif row['sentiment_index'] < 0 and row['æœ¬ç§‘å°±ä¸šç‡'] > 88:
            return 'Underrated'
        else:
            return 'Matched'
    
    df_merged['deviation_type'] = df_merged.apply(classify_deviation, axis=1)
    
    print(f"  âœ… Integrated {len(df_merged)} majors")
    print(f"  Deviation distribution:\n{df_merged['deviation_type'].value_counts()}\n")
    
    return df_merged

# ==================== PART 5: æ ¸å¿ƒå¯è§†åŒ– ====================

def create_enhanced_visualizations(df_integrated):
    """åˆ›å»ºå¢å¼ºç‰ˆå¯è§†åŒ– - å®Œæ•´ä¿®å¤ç‰ˆ"""
    
    if df_integrated is None or len(df_integrated) == 0:
        print("âš ï¸ No integrated data available")
        return
    
    print("="*70)
    print("ğŸ¨ Creating Enhanced Visualizations")
    print("="*70 + "\n")
    
    # ========== å›¾5: é™æ€æ°”æ³¡å›¾ ==========
    print("Creating bubble chart...")
    
    fig, ax = plt.subplots(figsize=(18, 12))
    
    # é…è‰²æ–¹æ¡ˆ
    color_map = {
        'Matched': '#2ecc71',
        'Overrated': '#e74c3c',
        'Underrated': '#3498db'
    }
    
    marker_map = {
        'Matched': 'o',
        'Overrated': '^',
        'Underrated': 's'
    }
    
    # ç»˜åˆ¶æ°”æ³¡
    for deviation_type in ['Matched', 'Overrated', 'Underrated']:
        df_type = df_integrated[df_integrated['deviation_type'] == deviation_type]
        
        if len(df_type) > 0:
            ax.scatter(
                df_type['æœ¬ç§‘å°±ä¸šç‡'],
                df_type['sentiment_index'],
                s=df_type['comment_count'] * 3,
                c=color_map[deviation_type],
                alpha=0.6,
                edgecolors='black',
                linewidth=2,
                marker=marker_map[deviation_type],
                label=deviation_type,
                zorder=3
            )
            
            # æ·»åŠ ä¸“ä¸šæ ‡ç­¾
            for idx, row in df_type.iterrows():
                ax.annotate(
                    row['major'],
                    (row['æœ¬ç§‘å°±ä¸šç‡'], row['sentiment_index']),
                    xytext=(6, 6),
                    textcoords='offset points',
                    fontsize=10,
                    fontweight='bold',
                    bbox=dict(
                        boxstyle='round,pad=0.4',
                        facecolor=color_map[deviation_type],
                        alpha=0.3,
                        edgecolor='black',
                        linewidth=0.8
                    ),
                    zorder=4
                )
    
    # å‚è€ƒçº¿
    ax.axhline(y=0, color='gray', linestyle='--', linewidth=1.5, alpha=0.5, zorder=1)
    ax.axvline(x=85, color='gray', linestyle='--', linewidth=1.5, alpha=0.5, zorder=1)
    ax.plot([65, 100], [-40, 50], 'k--', linewidth=2, alpha=0.3, 
            label='Ideal Match', zorder=2)
    
    # è±¡é™æ ‡æ³¨ - âœ… ä¿®å¤è¿™é‡Œçš„styleå‚æ•°
    ax.text(72, 45, 'HIGH Sentiment\nLOW Employment\n(Overrated)', 
           fontsize=12, alpha=0.6, fontstyle='italic', ha='center',
           bbox=dict(boxstyle='round,pad=0.5', facecolor='#e74c3c', alpha=0.2))
    
    ax.text(93, -35, 'LOW Sentiment\nHIGH Employment\n(Underrated)', 
           fontsize=12, alpha=0.6, fontstyle='italic', ha='center',
           bbox=dict(boxstyle='round,pad=0.5', facecolor='#3498db', alpha=0.2))
    
    ax.text(93, 40, 'HIGH Sentiment\nHIGH Employment\n(Ideal)', 
           fontsize=12, alpha=0.6, fontstyle='italic', ha='center',
           bbox=dict(boxstyle='round,pad=0.5', facecolor='#2ecc71', alpha=0.2))
    
    # è½´æ ‡ç­¾
    ax.set_xlabel('Official Employment Rate (%)', fontsize=14, fontweight='bold')
    ax.set_ylabel('Social Media Sentiment Index\n(Positive% - Negative%)', 
                 fontsize=14, fontweight='bold')
    ax.set_title('Zhang Xuefeng Major Recommendation: Sentiment vs Reality\n' +
                'Bubble Chart Analysis (BERT + Employment Data)',
                fontsize=17, fontweight='bold', pad=20)
    
    ax.set_xlim(65, 100)
    ax.set_ylim(-50, 60)
    ax.grid(True, alpha=0.3, linestyle=':', linewidth=0.5, zorder=0)
    ax.legend(loc='upper left', fontsize=12, framealpha=0.95, 
             edgecolor='black', shadow=True)
    
    plt.tight_layout()
    plt.savefig('./output/figures/05_bubble_chart.png', 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    print("âœ… Figure 5: Bubble Chart")
    
    # ========== å›¾6: è–ªèµ„å¯¹æ¯” ==========
    print("Creating salary comparison...")
    
    fig, axes = plt.subplots(1, 2, figsize=(16, 7))
    
    df_plot = df_integrated.sort_values('æœ¬ç¡•è–ªèµ„å·®', ascending=False).head(12)
    
    x = np.arange(len(df_plot))
    width = 0.35
    
    bars1 = axes[0].bar(x - width/2, df_plot['æœ¬ç§‘æœˆè–ª'], width, 
                       label='Bachelor', color='#3498db', alpha=0.8, edgecolor='black')
    bars2 = axes[0].bar(x + width/2, df_plot['ç¡•å£«æœˆè–ª'], width,
                       label='Master', color='#e74c3c', alpha=0.8, edgecolor='black')
    
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            if not np.isnan(height):
                axes[0].text(bar.get_x() + bar.get_width()/2., height,
                           f'{int(height)}',
                           ha='center', va='bottom', fontsize=9)
    
    axes[0].set_ylabel('Monthly Salary (CNY)', fontsize=12)
    axes[0].set_title('Salary Comparison: Bachelor vs Master', fontsize=14, fontweight='bold')
    axes[0].set_xticks(x)
    axes[0].set_xticklabels(df_plot['major'], rotation=45, ha='right')
    axes[0].legend(fontsize=11)
    axes[0].grid(axis='y', alpha=0.3)
    
    # å­¦å†æº¢ä»·ç‡
    df_plot2 = df_integrated.sort_values('å­¦å†è–ªèµ„æº¢ä»·ç‡%', ascending=False).head(12)
    colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(df_plot2)))
    bars = axes[1].barh(df_plot2['major'], df_plot2['å­¦å†è–ªèµ„æº¢ä»·ç‡%'], 
                        color=colors, alpha=0.8, edgecolor='black')
    
    axes[1].set_xlabel('Salary Premium Rate (%)', fontsize=12)
    axes[1].set_title('Education Premium: Master vs Bachelor', fontsize=14, fontweight='bold')
    axes[1].invert_yaxis()
    axes[1].grid(axis='x', alpha=0.3)
    
    for i, (idx, row) in enumerate(df_plot2.iterrows()):
        axes[1].text(row['å­¦å†è–ªèµ„æº¢ä»·ç‡%'], i, f" {row['å­¦å†è–ªèµ„æº¢ä»·ç‡%']:.0f}%",
                    va='center', fontsize=10, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('./output/figures/06_salary_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… Figure 6: Salary Comparison")
    
    # ========== å›¾7: å°±ä¸šç‡æå‡ ==========
    print("Creating employment improvement chart...")
    
    fig, ax = plt.subplots(figsize=(12, 8))
    
    df_plot3 = df_integrated.sort_values('å­¦å†å°±ä¸šç‡æå‡%', ascending=True).head(15)
    colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in df_plot3['å­¦å†å°±ä¸šç‡æå‡%']]
    bars = ax.barh(df_plot3['major'], df_plot3['å­¦å†å°±ä¸šç‡æå‡%'], 
                   color=colors, alpha=0.7, edgecolor='black')
    
    ax.set_xlabel('Employment Rate Improvement (%)', fontsize=12)
    ax.set_title('Employment Rate Boost: Master vs Bachelor', 
                fontsize=14, fontweight='bold')
    ax.axvline(x=0, color='black', linestyle='-', linewidth=1)
    ax.grid(axis='x', alpha=0.3)
    
    for i, (idx, row) in enumerate(df_plot3.iterrows()):
        ax.text(row['å­¦å†å°±ä¸šç‡æå‡%'], i, f" {row['å­¦å†å°±ä¸šç‡æå‡%']:.1f}%",
                va='center', fontsize=9, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('./output/figures/07_employment_improvement.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… Figure 7: Employment Improvement")
    
    print("\n" + "="*70)
    print("âœ… All enhanced visualizations completed!")
    print("="*70 + "\n")



def create_all_visualizations(df_comments, df_sentiment, df_integrated):
    """ç”Ÿæˆæ‰€æœ‰æ ¸å¿ƒå›¾è¡¨"""
    
    print("="*70)
    print("ğŸ“Š Creating Visualizations")
    print("="*70 + "\n")
    
    # å›¾1: å¹³å°åˆ†å¸ƒ
    fig1, ax1 = plt.subplots(figsize=(10, 6))
    platform_counts = df_comments['platform'].value_counts()
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    bars = ax1.bar(platform_counts.index, platform_counts.values, color=colors, alpha=0.8, edgecolor='black')
    ax1.set_title('Comment Distribution by Platform', fontsize=15, fontweight='bold')
    ax1.set_ylabel('Number of Comments')
    ax1.grid(axis='y', alpha=0.3)
    for bar in bars:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height):,}', ha='center', va='bottom', fontweight='bold')
    plt.tight_layout()
    plt.savefig('./output/figures/01_platform_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… Figure 1: Platform Distribution")
    
    # å›¾2: BERTæƒ…æ„Ÿåˆ†å¸ƒ
    fig2, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    sentiment_counts = df_comments['sentiment'].value_counts()
    colors_pie = ['#2ecc71', '#3498db', '#e74c3c']
    explode = [0.05 if i == sentiment_counts.idxmax() else 0 for i in sentiment_counts.index]
    
    axes[0].pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%',
               colors=colors_pie, explode=explode, shadow=True, startangle=90,
               textprops={'fontsize': 11, 'fontweight': 'bold'})
    axes[0].set_title('Overall Sentiment Distribution (BERT)', fontsize=14, fontweight='bold')
    
    sentiment_conf = df_comments.groupby('sentiment')['confidence'].mean()
    bars = axes[1].bar(sentiment_conf.index, sentiment_conf.values,
                       color=colors_pie, alpha=0.8, edgecolor='black')
    axes[1].set_title('Average BERT Confidence by Sentiment', fontsize=14, fontweight='bold')
    axes[1].set_ylabel('Confidence Score')
    axes[1].set_ylim([0, 1])
    axes[1].grid(axis='y', alpha=0.3)
    
    for bar in bars:
        height = bar.get_height()
        axes[1].text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.3f}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('./output/figures/02_bert_sentiment_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… Figure 2: BERT Sentiment Distribution")
    
    # å›¾3: ä¸“ä¸šæ¨èåº¦æ’å
    top_majors = df_sentiment.head(15)
    
    fig3, ax3 = plt.subplots(figsize=(12, 8))
    colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(top_majors)))
    bars = ax3.barh(top_majors['major'], top_majors['recommendation_score'], color=colors, edgecolor='black')
    
    ax3.set_xlabel('Recommendation Score', fontsize=12)
    ax3.set_title('Top 15 Majors - Social Media Recommendation Score\n(Based on BERT Sentiment Analysis)',
                 fontsize=14, fontweight='bold')
    ax3.invert_yaxis()
    ax3.grid(axis='x', alpha=0.3)
    
    for i, (idx, row) in enumerate(top_majors.iterrows()):
        ax3.text(row['recommendation_score'], i, f" {row['recommendation_score']:.1f}",
                va='center', fontsize=10, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('./output/figures/03_recommendation_ranking.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… Figure 3: Recommendation Ranking")
    
    # å›¾4: æƒ…æ„ŸæŒ‡æ•°å¯¹æ¯”
    top15 = df_sentiment.head(15)
    
    fig4, ax4 = plt.subplots(figsize=(14, 8))
    
    x = np.arange(len(top15))
    width = 0.35
    
    ax4.bar(x - width/2, top15['positive_rate'], width, label='Positive %', 
           color='#2ecc71', alpha=0.8)
    ax4.bar(x + width/2, top15['negative_rate'], width, label='Negative %',
           color='#e74c3c', alpha=0.8)
    
    ax4.set_ylabel('Percentage', fontsize=12)
    ax4.set_title('Top 15 Majors - Positive vs Negative Sentiment Rate',
                 fontsize=14, fontweight='bold')
    ax4.set_xticks(x)
    ax4.set_xticklabels(top15['major'], rotation=45, ha='right')
    ax4.legend()
    ax4.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('./output/figures/04_sentiment_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… Figure 4: Sentiment Comparison")
    
    # å›¾5: èˆ†æƒ… vs å°±ä¸šï¼ˆå¦‚æœæœ‰å°±ä¸šæ•°æ®ï¼‰
    if df_integrated is not None and len(df_integrated) > 0:
        employment_col = 'å°±ä¸šç‡' if 'å°±ä¸šç‡' in df_integrated.columns else 'employment_rate'
        
        if employment_col in df_integrated.columns:
            fig5 = px.scatter(
                df_integrated,
                x=employment_col,
                y='sentiment_index',
                size='comment_count',
                color='positive_rate',
                hover_data=['major'],
                text='major',
                color_continuous_scale='RdYlGn',
                title='<b>Social Media Sentiment vs Official Employment Rate</b><br>' +
                      '<sub>Size = Discussion Volume | Color = Positive Sentiment Rate</sub>',
                labels={
                    employment_col: 'Official Employment Rate (%)',
                    'sentiment_index': 'Social Media Sentiment Index',
                    'positive_rate': 'Positive %'
                }
            )
            
            fig5.update_traces(textposition='top center', textfont_size=8)
            fig5.update_layout(width=1400, height=800)
            
            fig5.write_html('./output/figures/05_sentiment_vs_employment.html')
            print("âœ… Figure 5: Sentiment vs Employment")
    
    print("\n" + "="*70)
    print("âœ… All visualizations completed!")
    print("="*70 + "\n")

# ==================== PART 6: å¯¼å‡ºæ•°æ®è¡¨ ====================

def export_all_tables(df_comments, df_sentiment, df_integrated):
    """å¯¼å‡ºæ‰€æœ‰æ•°æ®è¡¨"""
    
    print("ğŸ’¾ Exporting data tables...\n")
    
    # Table 1: åŸå§‹è¯„è®ºæ•°æ®ï¼ˆå¸¦BERTç»“æœï¼‰
    df_comments.to_csv('./output/tables/01_bert_analyzed_comments.csv', 
                      index=False, encoding='utf-8-sig')
    print("âœ… Table 1: BERT Analyzed Comments")
    
    # Table 2: ä¸“ä¸šèˆ†æƒ…æ±‡æ€»
    df_sentiment.to_csv('./output/tables/02_major_sentiment_summary.csv',
                       index=False, encoding='utf-8-sig')
    print("âœ… Table 2: Major Sentiment Summary")
    
    # Table 3: æ•´åˆæ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
    if df_integrated is not None and len(df_integrated) > 0:
        df_integrated.to_csv('./output/tables/03_integrated_sentiment_employment.csv',
                           index=False, encoding='utf-8-sig')
        print("âœ… Table 3: Integrated Analysis")
    
    print()

# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»æ‰§è¡Œæµç¨‹"""
    
    # Step 1: åŠ è½½è¯„è®ºæ•°æ®
    loader = RealDataLoader()
    loader.load_all_comments()
    df_comments = loader.standardize_comments()
    
    # Step 2: BERTæƒ…æ„Ÿåˆ†æ
    analyzer = RealBERTAnalyzer()
    sentiment_results = analyzer.batch_predict(df_comments['text'].tolist())
    
    df_comments['sentiment'] = [r[0] for r in sentiment_results]
    df_comments['confidence'] = [r[1] for r in sentiment_results]
    
    print("ğŸ“Š BERT Sentiment Analysis Results:")
    print(df_comments['sentiment'].value_counts())
    print(f"\nAverage Confidence: {df_comments['confidence'].mean():.3f}\n")
    
    # Step 3: æå–ä¸“ä¸šæåŠ
    df_with_majors = extract_majors_from_comments(df_comments)
    
    # Step 4: æŒ‰ä¸“ä¸šèšåˆ
    df_sentiment = aggregate_sentiment_by_major(df_with_majors)
    
    print("ğŸ“Š Top 10 Recommended Majors (by sentiment):")
    print(df_sentiment[['major', 'recommendation_score', 'positive_rate', 'comment_count']].head(10))
    print()
    
    # Step 5: åŠ è½½å°±ä¸šæ•°æ®
    df_employment = load_employment_data()
    
    # Step 6: æ•´åˆæ•°æ®
    df_integrated = None
    if df_employment is not None:
        df_integrated = integrate_sentiment_and_employment(df_sentiment, df_employment)
    
    # Step 7: ç”ŸæˆåŸºç¡€å¯è§†åŒ–
    create_all_visualizations(df_comments, df_sentiment, df_integrated)
    
    create_enhanced_visualizations(df_integrated)
    
    # Step 9: å¯¼å‡ºæ•°æ®è¡¨
    export_all_tables(df_comments, df_sentiment, df_integrated)
    
    # æœ€ç»ˆæ€»ç»“
    print("="*70)
    print("âœ… ANALYSIS COMPLETED SUCCESSFULLY!")
    print("="*70)
    print(f"ğŸ“Š Total comments analyzed: {len(df_comments):,}")
    print(f"ğŸ¤– BERT model used: uer/roberta-base-finetuned-jd-binary-chinese")
    print(f"ğŸ“ˆ Majors extracted: {len(df_sentiment)}")
    print(f"ğŸ“ Output directory: ./output/")
    print("="*70 + "\n")
    
    return df_comments, df_sentiment, df_integrated

# ==================== æ‰§è¡Œ ====================

if __name__ == "__main__":
    df_comments, df_sentiment, df_integrated = main()
