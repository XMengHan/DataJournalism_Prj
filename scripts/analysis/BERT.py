"""
å¼ é›ªå³°ä¸“ä¸šæ¨èç»¼åˆåˆ†æç³»ç»Ÿ - å®Œæ•´ç‰ˆï¼ˆå«Contentsåˆ†æï¼‰
çœŸå®BERTæ¨¡å‹ + çœŸå®æ•°æ® + å®Œæ•´å¯¹æ¯”åˆ†æ
æ–°å¢ï¼šå¾®åš/çŸ¥ä¹ contents + Bç«™ videos æ•°æ®çº³å…¥åˆ†æ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

# BERTæ¨¡å‹
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
    BERT_AVAILABLE = True
    print("âœ… BERT libraries loaded")
except ImportError:
    BERT_AVAILABLE = False
    print("âŒ Please install: pip install transformers torch")
    exit()

# å­—ä½“é…ç½®
import platform
import os

system = platform.system()
if system == 'Windows':
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei']
    FONT_PATH = 'C:\\Windows\\Fonts\\msyh.ttc'
elif system == 'Darwin':
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'STHeiti']
    FONT_PATH = '/System/Library/Fonts/STHeiti Light.ttc'
else:
    plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei']
    FONT_PATH = '/usr/share/fonts/truetype/wqy/wqy-microhei.ttc'

plt.rcParams['axes.unicode_minus'] = False

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs('./output/figures', exist_ok=True)
os.makedirs('./output/tables', exist_ok=True)

print("\n" + "="*70)
print("å¼ é›ªå³°ä¸“ä¸šæ¨è - èˆ†æƒ…åˆ†æ vs å°±ä¸šç°å® ç»¼åˆåˆ†æç³»ç»Ÿ")
print("="*70 + "\n")

# ==================== PART 1: æ•°æ®åŠ è½½ï¼ˆå«Contentsï¼‰ ====================

class RealDataLoader:
    """åŠ è½½çœŸå®æ•°æ®ï¼ˆè¯„è®º + å†…å®¹ï¼‰"""
    
    def __init__(self):
        self.base_path = './data/raw/'
        self.comments = {}
        self.contents = {}
        
    def load_all_comments(self):
        """åŠ è½½æ‰€æœ‰å¹³å°è¯„è®º"""
        
        print("ğŸ“¥ Loading Real Comment Data...")
        
        # çŸ¥ä¹è¯„è®º
        try:
            zhihu = pd.read_csv(f'{self.base_path}zhihu/search_comments_2025-12-09.csv')
            zhihu['platform'] = 'Zhihu'
            zhihu['data_type'] = 'comment'
            self.comments['zhihu'] = zhihu
            print(f"  âœ… Zhihu Comments: {len(zhihu):,} records")
        except Exception as e:
            print(f"  âš ï¸ Zhihu comments failed: {e}")
        
        # å¾®åšè¯„è®º
        try:
            weibo = pd.read_csv(f'{self.base_path}weibo/search_comments_2025-12-09.csv')
            weibo['platform'] = 'Weibo'
            weibo['data_type'] = 'comment'
            self.comments['weibo'] = weibo
            print(f"  âœ… Weibo Comments: {len(weibo):,} records")
        except Exception as e:
            print(f"  âš ï¸ Weibo comments failed: {e}")
        
        # Bç«™è¯„è®º
        try:
            bili = pd.read_csv(f'{self.base_path}bili/search_comments_2025-12-09.csv')
            bili['platform'] = 'Bilibili'
            bili['data_type'] = 'comment'
            self.comments['bili'] = bili
            print(f"  âœ… Bilibili Comments: {len(bili):,} records")
        except Exception as e:
            print(f"  âš ï¸ Bilibili comments failed: {e}")
        
        return self.comments
    
    def load_all_contents(self):
        """åŠ è½½æ‰€æœ‰å¹³å°çš„å†…å®¹/å¸–å­æ•°æ®"""
        
        print("\nğŸ“¥ Loading Content/Post Data...")
        
        # çŸ¥ä¹å†…å®¹
        try:
            zhihu_content = pd.read_csv(f'{self.base_path}zhihu/search_contents_2025-12-09.csv')
            zhihu_content['platform'] = 'Zhihu'
            zhihu_content['data_type'] = 'content'
            self.contents['zhihu'] = zhihu_content
            print(f"  âœ… Zhihu Contents: {len(zhihu_content):,} records")
        except Exception as e:
            print(f"  âš ï¸ Zhihu contents failed: {e}")
        
        # å¾®åšå†…å®¹
        try:
            weibo_content = pd.read_csv(f'{self.base_path}weibo/search_contents_2025-12-09.csv')
            weibo_content['platform'] = 'Weibo'
            weibo_content['data_type'] = 'content'
            self.contents['weibo'] = weibo_content
            print(f"  âœ… Weibo Contents: {len(weibo_content):,} records")
        except Exception as e:
            print(f"  âš ï¸ Weibo contents failed: {e}")
        
        # Bç«™è§†é¢‘
        try:
            bili_videos = pd.read_csv(f'{self.base_path}bili/search_videos_2025-12-09.csv')
            bili_videos['platform'] = 'Bilibili'
            bili_videos['data_type'] = 'video'
            self.contents['bili'] = bili_videos
            print(f"  âœ… Bilibili Videos: {len(bili_videos):,} records")
        except Exception as e:
            print(f"  âš ï¸ Bilibili videos failed: {e}")
        
        return self.contents
    
    def standardize_comments(self):
        """æ ‡å‡†åŒ–è¯„è®ºæ•°æ®"""
        
        print("\nğŸ”„ Standardizing comment data...")
        
        unified = []
        
        for platform, df in self.comments.items():
            df_copy = df.copy()
            
            # ç»Ÿä¸€æ–‡æœ¬å­—æ®µ
            text_fields = ['content', 'comment_text', 'text', 'comment_content']
            for field in text_fields:
                if field in df_copy.columns:
                    df_copy['text'] = df_copy[field].fillna('')
                    break
            
            if 'text' not in df_copy.columns:
                df_copy['text'] = ''
            
            # ç»Ÿä¸€ç‚¹èµå­—æ®µ
            like_fields = ['like_count', 'likes', 'digg_count', 'attitudes_count', 'liked_count']
            for field in like_fields:
                if field in df_copy.columns:
                    df_copy['likes'] = pd.to_numeric(df_copy[field], errors='coerce').fillna(0)
                    break
            
            if 'likes' not in df_copy.columns:
                df_copy['likes'] = 0
            
            # é€‰æ‹©æ ¸å¿ƒå­—æ®µ
            df_copy = df_copy[['text', 'likes', 'platform', 'data_type']].copy()
            unified.append(df_copy)
        
        df_all = pd.concat(unified, ignore_index=True)
        
        # æ•°æ®æ¸…æ´—
        df_all = df_all[
            (df_all['text'].notna()) &
            (df_all['text'].str.len() > 10) &
            (df_all['text'] != '')
        ].copy()
        
        df_all['text_length'] = df_all['text'].str.len()
        
        print(f"  âœ… Total valid comments: {len(df_all):,}")
        print(f"  Platform distribution:\n{df_all['platform'].value_counts()}\n")
        
        return df_all
    
    def standardize_contents(self):
        """æ ‡å‡†åŒ–å†…å®¹/å¸–å­æ•°æ®"""
        
        print("ğŸ”„ Standardizing content data...")
        
        unified = []
        
        for platform, df in self.contents.items():
            df_copy = df.copy()
            
            # æ ¹æ®å¹³å°é€‰æ‹©æ–‡æœ¬å­—æ®µ
            if platform == 'zhihu':
                # çŸ¥ä¹ï¼šåˆå¹¶ title + content_text + desc
                title = df_copy.get('title', pd.Series([''] * len(df_copy))).fillna('')
                content_text = df_copy.get('content_text', pd.Series([''] * len(df_copy))).fillna('')
                desc = df_copy.get('desc', pd.Series([''] * len(df_copy))).fillna('')
                df_copy['text'] = title.astype(str) + ' ' + content_text.astype(str) + ' ' + desc.astype(str)
                
                # ç‚¹èµæ•°
                df_copy['likes'] = pd.to_numeric(df_copy.get('voteup_count', 0), errors='coerce').fillna(0)
                
            elif platform == 'weibo':
                # å¾®åšï¼šä½¿ç”¨ content å­—æ®µ
                df_copy['text'] = df_copy.get('content', pd.Series([''] * len(df_copy))).fillna('')
                df_copy['likes'] = pd.to_numeric(df_copy.get('liked_count', 0), errors='coerce').fillna(0)
                
            elif platform == 'bili':
                # Bç«™è§†é¢‘ï¼šåˆå¹¶ title + desc
                title = df_copy.get('title', pd.Series([''] * len(df_copy))).fillna('')
                desc = df_copy.get('desc', pd.Series([''] * len(df_copy))).fillna('')
                df_copy['text'] = title.astype(str) + ' ' + desc.astype(str)
                df_copy['likes'] = pd.to_numeric(df_copy.get('liked_count', 0), errors='coerce').fillna(0)
            
            # é€‰æ‹©æ ¸å¿ƒå­—æ®µ
            df_copy = df_copy[['text', 'likes', 'platform', 'data_type']].copy()
            unified.append(df_copy)
        
        if not unified:
            print("  âš ï¸ No content data found")
            return pd.DataFrame()
        
        df_all = pd.concat(unified, ignore_index=True)
        
        # æ•°æ®æ¸…æ´—
        df_all = df_all[
            (df_all['text'].notna()) &
            (df_all['text'].str.len() > 10) &
            (df_all['text'] != '')
        ].copy()
        
        df_all['text_length'] = df_all['text'].str.len()
        
        print(f"  âœ… Total valid contents: {len(df_all):,}")
        print(f"  Platform distribution:\n{df_all['platform'].value_counts()}")
        print(f"  Data type distribution:\n{df_all['data_type'].value_counts()}\n")
        
        return df_all
    
    def merge_all_data(self, df_comments, df_contents):
        """åˆå¹¶è¯„è®ºå’Œå†…å®¹æ•°æ®"""
        
        print("ğŸ”— Merging comments and contents...")
        
        df_all = pd.concat([df_comments, df_contents], ignore_index=True)
        
        print(f"  âœ… Total merged records: {len(df_all):,}")
        print(f"  By data type:\n{df_all['data_type'].value_counts()}")
        print(f"  By platform:\n{df_all['platform'].value_counts()}\n")
        
        return df_all


def load_employment_data():
    """åŠ è½½å°±ä¸šæ•°æ®"""
    
    print("ğŸ“¥ Loading Employment Data...")
    
    try:
        df_emp = pd.read_csv('./data/processed/comprehensive_major_data.csv')
        print(f"  âœ… Loaded {len(df_emp)} majors' employment data")
        print(f"  Columns: {df_emp.columns.tolist()}\n")
        return df_emp
    except Exception as e:
        print(f"  âŒ Failed to load employment data: {e}")
        return None


# ==================== PART 2: çœŸå®BERTæƒ…æ„Ÿåˆ†æ ====================

class RealBERTAnalyzer:
    """çœŸå®çš„BERTä¸­æ–‡æƒ…æ„Ÿåˆ†æ"""
    
    def __init__(self):
        print("ğŸ¤– Loading BERT Model...")
        
        try:
            # ä½¿ç”¨ç»è¿‡å¾®è°ƒçš„ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹
            self.tokenizer = AutoTokenizer.from_pretrained(
                "uer/roberta-base-finetuned-jd-binary-chinese"
            )
            self.model = AutoModelForSequenceClassification.from_pretrained(
                "uer/roberta-base-finetuned-jd-binary-chinese"
            )
            
            # æ£€æŸ¥GPU
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.model.to(self.device)
            self.model.eval()
            
            device_name = "GPU" if torch.cuda.is_available() else "CPU"
            print(f"  âœ… BERT Model Loaded (Device: {device_name})\n")
            
        except Exception as e:
            print(f"  âŒ BERT loading failed: {e}")
            self.model = None
    
    def predict_single(self, text):
        """é¢„æµ‹å•æ¡æ–‡æœ¬"""
        
        if self.model is None:
            return 'neutral', 0.5
        
        try:
            # æˆªæ–­å¹¶tokenize
            text = str(text)[:512]
            inputs = self.tokenizer(text, return_tensors='pt', truncation=True, 
                                   padding=True, max_length=512)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # é¢„æµ‹
            with torch.no_grad():
                outputs = self.model(**inputs)
                probs = torch.nn.functional.softmax(outputs.logits, dim=1)
            
            # è§£æç»“æœ
            positive_prob = probs[0][1].item()
            negative_prob = probs[0][0].item()
            
            if positive_prob > 0.6:
                return 'positive', positive_prob
            elif negative_prob > 0.6:
                return 'negative', negative_prob
            else:
                return 'neutral', max(positive_prob, negative_prob)
        
        except Exception as e:
            return 'neutral', 0.5
    
    def batch_predict(self, texts, batch_size=32):
        """æ‰¹é‡é¢„æµ‹"""
        
        results = []
        total = len(texts)
        
        print(f"ğŸ”„ Analyzing {total:,} texts with BERT...")
        
        for i in range(0, total, batch_size):
            batch = texts[i:i+batch_size]
            batch_results = [self.predict_single(text) for text in batch]
            results.extend(batch_results)
            
            if (i + batch_size) % 500 == 0 or i + batch_size >= total:
                progress = min(i + batch_size, total)
                print(f"  Progress: {progress:,}/{total:,} ({progress/total*100:.1f}%)")
        
        print("  âœ… BERT analysis completed!\n")
        return results


# ==================== PART 3: ä¸“ä¸šæå–ä¸åŒ¹é… ====================

# 113ä¸ªæœ¬ç§‘ä¸“ä¸šå…³é”®è¯åº“ï¼ˆæ‰©å±•ç‰ˆï¼‰
MAJOR_KEYWORDS = {
    'è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯': ['è®¡ç®—æœº', 'CS', 'è½¯ä»¶', 'ç¨‹åº', 'ç å†œ', 'IT', 'ç¼–ç¨‹', 'ä»£ç ', 'è®¡ç§‘'],
    'è½¯ä»¶å·¥ç¨‹': ['è½¯ä»¶å·¥ç¨‹', 'è½¯å·¥', 'å¼€å‘', 'ç¨‹åºå‘˜'],
    'ç”µå­ä¿¡æ¯å·¥ç¨‹': ['ç”µå­ä¿¡æ¯', 'ç”µä¿¡', 'é€šä¿¡', 'ä¿¡å·', 'ç”µå­å·¥ç¨‹'],
    'ä¸´åºŠåŒ»å­¦': ['ä¸´åºŠ', 'åŒ»å­¦', 'åŒ»ç”Ÿ', 'åŒ»å¸ˆ', 'å­¦åŒ»', 'åŒ»å­¦ç”Ÿ'],
    'é‡‘èå­¦': ['é‡‘è', 'æŠ•èµ„', 'é“¶è¡Œ', 'è¯åˆ¸', 'åŸºé‡‘', 'é‡‘èå­¦'],
    'ä¼šè®¡å­¦': ['ä¼šè®¡', 'è´¢åŠ¡', 'å®¡è®¡', 'CPA', 'è´¢ä¼š'],
    'æ³•å­¦': ['æ³•å­¦', 'æ³•å¾‹', 'å¾‹å¸ˆ', 'å¸æ³•', 'æ³•è€ƒ', 'æ³•ç¡•'],
    'åœŸæœ¨å·¥ç¨‹': ['åœŸæœ¨', 'å»ºç­‘', 'æ–½å·¥', 'å·¥ç¨‹', 'åœŸå»º', 'åœŸæœ¨å·¥ç¨‹'],
    'æœºæ¢°å·¥ç¨‹': ['æœºæ¢°', 'åˆ¶é€ ', 'æœºç”µ', 'æœºæ¢°å·¥ç¨‹', 'æœºæ¢°è®¾è®¡'],
    'ç”µæ°”å·¥ç¨‹': ['ç”µæ°”', 'ç”µåŠ›', 'å¼ºç”µ', 'ç”µæ°”å·¥ç¨‹', 'ç”µå·¥'],
    'è‡ªåŠ¨åŒ–': ['è‡ªåŠ¨åŒ–', 'æ§åˆ¶', 'è‡ªåŠ¨æ§åˆ¶'],
    'é€šä¿¡å·¥ç¨‹': ['é€šä¿¡å·¥ç¨‹', 'é€šä¿¡', '5G', 'ç½‘ç»œé€šä¿¡'],
    'å¸ˆèŒƒç±»': ['å¸ˆèŒƒ', 'æ•™è‚²', 'æ•™å¸ˆ', 'è€å¸ˆ', 'å½“è€å¸ˆ', 'æ•™è‚²å­¦'],
    'æŠ¤ç†å­¦': ['æŠ¤ç†', 'æŠ¤å£«', 'æŠ¤ç†å­¦'],
    'è‹±è¯­': ['è‹±è¯­', 'è‹±æ–‡', 'ç¿»è¯‘', 'å¤–è¯­', 'è‹±è¯­ä¸“ä¸š'],
    'æ–°é—»å­¦': ['æ–°é—»', 'ä¼ æ’­', 'åª’ä½“', 'è®°è€…', 'æ–°é—»å­¦', 'ä¼ åª’'],
    'ç”Ÿç‰©å·¥ç¨‹': ['ç”Ÿç‰©', 'ç”Ÿå·¥', 'ç”ŸåŒ–', 'å¤©å‘', 'ç”Ÿç‰©å·¥ç¨‹', 'ç”Ÿç§‘'],
    'åŒ–å­¦ç±»': ['åŒ–å­¦', 'åŒ–å·¥', 'åŒ–å­¦å·¥ç¨‹'],
    'ææ–™ç±»': ['ææ–™', 'é«˜åˆ†å­', 'ææ–™ç§‘å­¦', 'ææ–™å·¥ç¨‹'],
    'ç¯å¢ƒå·¥ç¨‹': ['ç¯å¢ƒ', 'ç¯å·¥', 'ç¯ä¿', 'ç¯å¢ƒå·¥ç¨‹'],
    'ç»æµå­¦': ['ç»æµå­¦', 'ç»æµ', 'å®è§‚', 'å¾®è§‚'],
    'å·¥å•†ç®¡ç†': ['å·¥å•†ç®¡ç†', 'ç®¡ç†å­¦', 'ä¼ä¸šç®¡ç†', 'MBA'],
    'å¸‚åœºè¥é”€': ['å¸‚åœºè¥é”€', 'è¥é”€', 'é”€å”®'],
    'äººåŠ›èµ„æº': ['äººåŠ›èµ„æº', 'HR', 'äººäº‹'],
    'å»ºç­‘å­¦': ['å»ºç­‘å­¦', 'å»ºç­‘è®¾è®¡', 'å»ºç­‘å¸ˆ'],
    'æ•°å­¦': ['æ•°å­¦', 'æ•°å­¦ä¸“ä¸š', 'åº”ç”¨æ•°å­¦', 'æ•°å­¦ç³»'],
    'ç‰©ç†å­¦': ['ç‰©ç†', 'ç‰©ç†å­¦', 'ç‰©ç†ç³»'],
    'å¿ƒç†å­¦': ['å¿ƒç†', 'å¿ƒç†å­¦', 'å¿ƒç†å’¨è¯¢'],
    'æ±‰è¯­è¨€æ–‡å­¦': ['æ±‰è¯­è¨€', 'ä¸­æ–‡', 'æ–‡å­¦', 'ä¸­æ–‡ç³»', 'æ±‰è¯­'],
    'å†å²å­¦': ['å†å²', 'å†å²å­¦', 'è€ƒå¤'],
    'å“²å­¦': ['å“²å­¦', 'å“²å­¦ä¸“ä¸š'],
    'è‰ºæœ¯è®¾è®¡': ['è®¾è®¡', 'è‰ºæœ¯è®¾è®¡', 'å¹³é¢è®¾è®¡', 'UI'],
    'éŸ³ä¹': ['éŸ³ä¹', 'éŸ³ä¹ä¸“ä¸š', 'å£°ä¹'],
    'ç¾æœ¯': ['ç¾æœ¯', 'ç»‘å®š', 'ç¾æœ¯ç”Ÿ', 'ç”»ç”»'],
    'ä½“è‚²': ['ä½“è‚²', 'ä½“è‚²ä¸“ä¸š', 'ä½“è‚²ç”Ÿ'],
    'å†œå­¦': ['å†œå­¦', 'å†œä¸š', 'ç§æ¤'],
    'å…½åŒ»': ['å…½åŒ»', 'åŠ¨ç‰©åŒ»å­¦', 'å® ç‰©åŒ»ç”Ÿ'],
    'è¯å­¦': ['è¯å­¦', 'åˆ¶è¯', 'è¯å‰‚'],
    'ä¸­åŒ»å­¦': ['ä¸­åŒ»', 'ä¸­åŒ»å­¦', 'ä¸­è¯'],
    'å£è…”åŒ»å­¦': ['å£è…”', 'ç‰™åŒ»', 'å£è…”åŒ»å­¦'],
    'äººå·¥æ™ºèƒ½': ['äººå·¥æ™ºèƒ½', 'AI', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ '],
    'æ•°æ®ç§‘å­¦': ['æ•°æ®ç§‘å­¦', 'å¤§æ•°æ®', 'æ•°æ®åˆ†æ'],
    'ç½‘ç»œå®‰å…¨': ['ç½‘ç»œå®‰å…¨', 'ä¿¡æ¯å®‰å…¨', 'ç½‘å®‰'],
    'èˆªç©ºèˆªå¤©': ['èˆªç©ºèˆªå¤©', 'é£è¡Œå™¨', 'èˆªå¤©'],
}


def extract_majors_from_text(df):
    """ä»æ–‡æœ¬ä¸­æå–ä¸“ä¸šæåŠ"""
    
    print("ğŸ” Extracting major mentions from texts...")
    
    def find_majors(text):
        text_lower = str(text).lower()
        found = []
        for major, keywords in MAJOR_KEYWORDS.items():
            if any(kw in text_lower for kw in keywords):
                found.append(major)
        return found if found else ['æœªæ˜ç¡®æåŠ']
    
    df['mentioned_majors'] = df['text'].apply(find_majors)
    
    # å±•å¼€ä¸ºå¤šè¡Œ
    df_expanded = df.explode('mentioned_majors')
    df_expanded = df_expanded[df_expanded['mentioned_majors'] != 'æœªæ˜ç¡®æåŠ']
    
    print(f"  âœ… Extracted {len(df_expanded):,} major mentions\n")
    
    return df_expanded


def aggregate_sentiment_by_major(df):
    """æŒ‰ä¸“ä¸šèšåˆæƒ…æ„Ÿåˆ†æç»“æœ"""
    
    print("ğŸ“Š Aggregating sentiment by major...")
    
    major_sentiment = df.groupby('mentioned_majors').agg({
        'sentiment': lambda x: {
            'positive_rate': (x == 'positive').sum() / len(x) * 100,
            'negative_rate': (x == 'negative').sum() / len(x) * 100,
            'neutral_rate': (x == 'neutral').sum() / len(x) * 100
        },
        'confidence': 'mean',
        'text': 'count',
        'likes': 'sum'
    }).reset_index()
    
    # å±•å¼€æƒ…æ„Ÿå­—å…¸
    major_sentiment['positive_rate'] = major_sentiment['sentiment'].apply(lambda x: x['positive_rate'])
    major_sentiment['negative_rate'] = major_sentiment['sentiment'].apply(lambda x: x['negative_rate'])
    major_sentiment['neutral_rate'] = major_sentiment['sentiment'].apply(lambda x: x['neutral_rate'])
    major_sentiment = major_sentiment.drop('sentiment', axis=1)
    
    # é‡å‘½ååˆ—
    major_sentiment = major_sentiment.rename(columns={
        'mentioned_majors': 'major',
        'text': 'mention_count',
        'likes': 'total_likes'
    })
    
    # è®¡ç®—ç»¼åˆæŒ‡æ ‡
    major_sentiment['sentiment_index'] = (
        major_sentiment['positive_rate'] - major_sentiment['negative_rate']
    )
    
    max_mentions = major_sentiment['mention_count'].max()
    major_sentiment['recommendation_score'] = (
        major_sentiment['positive_rate'] * 
        major_sentiment['confidence'] * 
        (major_sentiment['mention_count'] / max_mentions) * 100
    )
    
    print(f"  âœ… Aggregated {len(major_sentiment)} majors\n")
    
    return major_sentiment.sort_values('recommendation_score', ascending=False)


def aggregate_sentiment_by_major_and_type(df):
    """æŒ‰ä¸“ä¸šå’Œæ•°æ®ç±»å‹åˆ†åˆ«èšåˆï¼ˆç”¨äºå¯¹æ¯”åˆ†æï¼‰"""
    
    print("ğŸ“Š Aggregating sentiment by major and data type...")
    
    result = df.groupby(['mentioned_majors', 'data_type']).agg({
        'sentiment': lambda x: (x == 'positive').sum() / len(x) * 100,
        'confidence': 'mean',
        'text': 'count',
        'likes': 'sum'
    }).reset_index()
    
    result = result.rename(columns={
        'mentioned_majors': 'major',
        'sentiment': 'positive_rate',
        'text': 'mention_count',
        'likes': 'total_likes'
    })
    
    print(f"  âœ… Aggregated {len(result)} major-type combinations\n")
    
    return result


# ==================== PART 4: æ•°æ®æ•´åˆ ====================

def integrate_sentiment_and_employment(df_sentiment, df_employment):
    """æ•´åˆèˆ†æƒ…æ•°æ®å’Œå°±ä¸šæ•°æ® - å®Œå…¨åŒ¹é…ç‰ˆ"""
    
    print("ğŸ”— Integrating sentiment and employment data...")
    print(f"  Sentiment columns: {df_sentiment.columns.tolist()}")
    print(f"  Employment columns: {df_employment.columns.tolist()}")
    
    # åˆ›å»ºä¸“ä¸šåç§°æ˜ å°„
    major_mapping = {}
    
    for sent_major in df_sentiment['major'].unique():
        # ç›´æ¥ç²¾ç¡®åŒ¹é…
        if sent_major in df_employment['ä¸“ä¸š'].values:
            major_mapping[sent_major] = sent_major
        else:
            # æ¨¡ç³ŠåŒ¹é…
            for emp_major in df_employment['ä¸“ä¸š'].values:
                # åŒå‘åŒ…å«åŒ¹é…
                if (sent_major in str(emp_major)) or (str(emp_major) in sent_major):
                    major_mapping[sent_major] = emp_major
                    break
    
    print(f"  âœ… Successfully mapped {len(major_mapping)} majors:")
    for k, v in list(major_mapping.items())[:5]:
        print(f"     {k} â†’ {v}")
    
    # åº”ç”¨æ˜ å°„
    df_sentiment['employment_major'] = df_sentiment['major'].map(major_mapping)
    
    # åˆå¹¶æ•°æ®
    df_merged = pd.merge(
        df_sentiment,
        df_employment,
        left_on='employment_major',
        right_on='ä¸“ä¸š',
        how='inner'
    )
    
    if len(df_merged) == 0:
        print("  âš ï¸ No matches found!")
        return None
    
    # æ•°æ®ç±»å‹è½¬æ¢
    df_merged['æœ¬ç§‘å°±ä¸šç‡'] = pd.to_numeric(df_merged['æœ¬ç§‘å°±ä¸šç‡'], errors='coerce') * 100
    df_merged['æœ¬ç§‘æœˆè–ª'] = pd.to_numeric(df_merged['æœ¬ç§‘æœˆè–ª'], errors='coerce')
    df_merged['ç¡•å£«å°±ä¸šç‡'] = pd.to_numeric(df_merged['ç¡•å£«å°±ä¸šç‡'], errors='coerce') * 100
    df_merged['ç¡•å£«æœˆè–ª'] = pd.to_numeric(df_merged['ç¡•å£«æœˆè–ª'], errors='coerce')
    df_merged['å­¦å†è–ªèµ„æº¢ä»·ç‡%'] = pd.to_numeric(df_merged['å­¦å†è–ªèµ„æº¢ä»·ç‡%'], errors='coerce')
    
    # è®¡ç®—æ’å
    df_merged['sentiment_rank'] = df_merged['sentiment_index'].rank(ascending=False)
    df_merged['employment_rank'] = df_merged['æœ¬ç§‘å°±ä¸šç‡'].rank(ascending=False)
    df_merged['deviation_score'] = abs(df_merged['sentiment_rank'] - df_merged['employment_rank'])
    
    # åˆ†ç±»æ ‡ç­¾
    def classify_deviation(row):
        if row['sentiment_index'] > 20 and row['æœ¬ç§‘å°±ä¸šç‡'] < 85:
            return 'Overrated'
        elif row['sentiment_index'] < 0 and row['æœ¬ç§‘å°±ä¸šç‡'] > 88:
            return 'Underrated'
        else:
            return 'Matched'
    
    df_merged['deviation_type'] = df_merged.apply(classify_deviation, axis=1)
    
    print(f"  âœ… Integrated {len(df_merged)} majors")
    print(f"  Deviation distribution:\n{df_merged['deviation_type'].value_counts()}\n")
    
    return df_merged


# ==================== PART 5: æ ¸å¿ƒå¯è§†åŒ– ====================

def create_enhanced_visualizations(df_integrated):
    """åˆ›å»ºå¢å¼ºç‰ˆå¯è§†åŒ–"""
    
    if df_integrated is None or len(df_integrated) == 0:
        print("âš ï¸ No integrated data available")
        return
    
    print("="*70)
    print("ğŸ¨ Creating Enhanced Visualizations")
    print("="*70 + "\n")
    
    # ========== å›¾5: é™æ€æ°”æ³¡å›¾ ==========
    print("Creating bubble chart...")
    
    fig, ax = plt.subplots(figsize=(18, 12))
    
    color_map = {
        'Matched': '#2ecc71',
        'Overrated': '#e74c3c',
        'Underrated': '#3498db'
    }
    
    marker_map = {
        'Matched': 'o',
        'Overrated': '^',
        'Underrated': 's'
    }
    
    for deviation_type in ['Matched', 'Overrated', 'Underrated']:
        df_type = df_integrated[df_integrated['deviation_type'] == deviation_type]
        
        if len(df_type) > 0:
            ax.scatter(
                df_type['æœ¬ç§‘å°±ä¸šç‡'],
                df_type['sentiment_index'],
                s=df_type['mention_count'] * 3,
                c=color_map[deviation_type],
                alpha=0.6,
                edgecolors='black',
                linewidth=2,
                marker=marker_map[deviation_type],
                label=deviation_type,
                zorder=3
            )
            
            for idx, row in df_type.iterrows():
                ax.annotate(
                    row['major'],
                    (row['æœ¬ç§‘å°±ä¸šç‡'], row['sentiment_index']),
                    xytext=(6, 6),
                    textcoords='offset points',
                    fontsize=10,
                    fontweight='bold',
                    bbox=dict(
                        boxstyle='round,pad=0.4',
                        facecolor=color_map[deviation_type],
                        alpha=0.3,
                        edgecolor='black',
                        linewidth=0.8
                    ),
                    zorder=4
                )
    
    ax.axhline(y=0, color='gray', linestyle='--', linewidth=1.5, alpha=0.5, zorder=1)
    ax.axvline(x=85, color='gray', linestyle='--', linewidth=1.5, alpha=0.5, zorder=1)
    ax.plot([65, 100], [-40, 50], 'k--', linewidth=2, alpha=0.3, 
            label='Ideal Match', zorder=2)
    
    ax.text(72, 45, 'HIGH Sentiment\nLOW Employment\n(Overrated)', 
           fontsize=12, alpha=0.6, fontstyle='italic', ha='center',
           bbox=dict(boxstyle='round,pad=0.5', facecolor='#e74c3c', alpha=0.2))
    
    ax.text(93, -35, 'LOW Sentiment\nHIGH Employment\n(Underrated)', 
           fontsize=12, alpha=0.6, fontstyle='italic', ha='center',
           bbox=dict(boxstyle='round,pad=0.5', facecolor='#3498db', alpha=0.2))
    
    ax.text(93, 40, 'HIGH Sentiment\nHIGH Employment\n(Ideal)', 
           fontsize=12, alpha=0.6, fontstyle='italic', ha='center',
           bbox=dict(boxstyle='round,pad=0.5', facecolor='#2ecc71', alpha=0.2))
    
    ax.set_xlabel('Official Employment Rate (%)', fontsize=14, fontweight='bold')
    ax.set_ylabel('Social Media Sentiment Index\n(Positive% - Negative%)', 
                 fontsize=14, fontweight='bold')
    ax.set_title('Zhang Xuefeng Major Recommendation: Sentiment vs Reality\n' +
                'Bubble Chart Analysis (BERT + Employment Data + Contents)',
                fontsize=17, fontweight='bold', pad=20)
    
    ax.set_xlim(65, 100)
    ax.set_ylim(-50, 60)
    ax.grid(True, alpha=0.3, linestyle=':', linewidth=0.5, zorder=0)
    ax.legend(loc='upper left', fontsize=12, framealpha=0.95, 
             edgecolor='black', shadow=True)
    
    plt.tight_layout()
    plt.savefig('./output/figures/05_bubble_chart.png', 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    print("âœ… Figure 5: Bubble Chart")
    
    # ========== å›¾6: è–ªèµ„å¯¹æ¯” ==========
    print("Creating salary comparison...")
    
    fig, axes = plt.subplots(1, 2, figsize=(16, 7))
    
    df_plot = df_integrated.sort_values('æœ¬ç¡•è–ªèµ„å·®', ascending=False).head(12)
    
    x = np.arange(len(df_plot))
    width = 0.35
    
    bars1 = axes[0].bar(x - width/2, df_plot['æœ¬ç§‘æœˆè–ª'], width, 
                       label='Bachelor', color='#3498db', alpha=0.8, edgecolor='black')
    bars2 = axes[0].bar(x + width/2, df_plot['ç¡•å£«æœˆè–ª'], width,
                       label='Master', color='#e74c3c', alpha=0.8, edgecolor='black')
    
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            if not np.isnan(height):
                axes[0].text(bar.get_x() + bar.get_width()/2., height,
                           f'{int(height)}',
                           ha='center', va='bottom', fontsize=9)
    
    axes[0].set_ylabel('Monthly Salary (CNY)', fontsize=12)
    axes[0].set_title('Salary Comparison: Bachelor vs Master', fontsize=14, fontweight='bold')
    axes[0].set_xticks(x)
    axes[0].set_xticklabels(df_plot['major'], rotation=45, ha='right')
    axes[0].legend(fontsize=11)
    axes[0].grid(axis='y', alpha=0.3)
    
    df_plot2 = df_integrated.sort_values('å­¦å†è–ªèµ„æº¢ä»·ç‡%', ascending=False).head(12)
    colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(df_plot2)))
    bars = axes[1].barh(df_plot2['major'], df_plot2['å­¦å†è–ªèµ„æº¢ä»·ç‡%'], 
                        color=colors, alpha=0.8, edgecolor='black')
    
    axes[1].set_xlabel('Salary Premium Rate (%)', fontsize=12)
    axes[1].set_title('Education Premium: Master vs Bachelor', fontsize=14, fontweight='bold')
    axes[1].invert_yaxis()
    axes[1].grid(axis='x', alpha=0.3)
    
    for i, (idx, row) in enumerate(df_plot2.iterrows()):
        axes[1].text(row['å­¦å†è–ªèµ„æº¢ä»·ç‡%'], i, f" {row['å­¦å†è–ªèµ„æº¢ä»·ç‡%']:.0f}%",
                    va='center', fontsize=10, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('./output/figures/06_salary_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… Figure 6: Salary Comparison")
    
    # ========== å›¾7: å°±ä¸šç‡æå‡ ==========
    print("Creating employment improvement chart...")
    
    fig, ax = plt.subplots(figsize=(12, 8))
    
    df_plot3 = df_integrated.sort_values('å­¦å†å°±ä¸šç‡æå‡%', ascending=True).head(15)
    colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in df_plot3['å­¦å†å°±ä¸šç‡æå‡%']]
    bars = ax.barh(df_plot3['major'], df_plot3['å­¦å†å°±ä¸šç‡æå‡%'], 
                   color=colors, alpha=0.7, edgecolor='black')
    
    ax.set_xlabel('Employment Rate Improvement (%)', fontsize=12)
    ax.set_title('Employment Rate Boost: Master vs Bachelor', 
                fontsize=14, fontweight='bold')
    ax.axvline(x=0, color='black', linestyle='-', linewidth=1)
    ax.grid(axis='x', alpha=0.3)
    
    for i, (idx, row) in enumerate(df_plot3.iterrows()):
        ax.text(row['å­¦å†å°±ä¸šç‡æå‡%'], i, f" {row['å­¦å†å°±ä¸šç‡æå‡%']:.1f}%",
                va='center', fontsize=9, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('./output/figures/07_employment_improvement.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… Figure 7: Employment Improvement")
    
    print("\n" + "="*70)
    print("âœ… All enhanced visualizations completed!")
    print("="*70 + "\n")


def create_all_visualizations(df_all, df_sentiment, df_integrated):
    """ç”Ÿæˆæ‰€æœ‰æ ¸å¿ƒå›¾è¡¨"""
    
    print("="*70)
    print("ğŸ“Š Creating Visualizations")
    print("="*70 + "\n")
    
    # å›¾1: å¹³å°å’Œæ•°æ®ç±»å‹åˆ†å¸ƒ
    fig1, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # æŒ‰å¹³å°åˆ†å¸ƒ
    platform_counts = df_all['platform'].value_counts()
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    bars = axes[0].bar(platform_counts.index, platform_counts.values, color=colors, alpha=0.8, edgecolor='black')
    axes[0].set_title('Distribution by Platform', fontsize=15, fontweight='bold')
    axes[0].set_ylabel('Number of Records')
    axes[0].grid(axis='y', alpha=0.3)
    for bar in bars:
        height = bar.get_height()
        axes[0].text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height):,}', ha='center', va='bottom', fontweight='bold')
    
    # æŒ‰æ•°æ®ç±»å‹åˆ†å¸ƒ
    type_counts = df_all['data_type'].value_counts()
    colors_type = ['#9b59b6', '#3498db', '#e74c3c']
    bars2 = axes[1].bar(type_counts.index, type_counts.values, color=colors_type[:len(type_counts)], alpha=0.8, edgecolor='black')
    axes[1].set_title('Distribution by Data Type', fontsize=15, fontweight='bold')
    axes[1].set_ylabel('Number of Records')
    axes[1].grid(axis='y', alpha=0.3)
    for bar in bars2:
        height = bar.get_height()
        axes[1].text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height):,}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('./output/figures/01_platform_type_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… Figure 1: Platform & Type Distribution")
    
    # å›¾2: BERTæƒ…æ„Ÿåˆ†å¸ƒï¼ˆæŒ‰æ•°æ®ç±»å‹ï¼‰
    fig2, axes = plt.subplots(1, 3, figsize=(16, 5))
    
    colors_pie = ['#2ecc71', '#3498db', '#e74c3c']
    
    for i, data_type in enumerate(df_all['data_type'].unique()):
        df_type = df_all[df_all['data_type'] == data_type]
        sentiment_counts = df_type['sentiment'].value_counts()
        
        axes[i].pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%',
                   colors=colors_pie, shadow=True, startangle=90,
                   textprops={'fontsize': 10, 'fontweight': 'bold'})
        axes[i].set_title(f'{data_type.title()} Sentiment\n(n={len(df_type):,})', 
                         fontsize=12, fontweight='bold')
    
    plt.suptitle('BERT Sentiment Distribution by Data Type', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig('./output/figures/02_bert_sentiment_by_type.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… Figure 2: BERT Sentiment by Data Type")
    
    # å›¾3: ä¸“ä¸šæ¨èåº¦æ’å
    top_majors = df_sentiment.head(15)
    
    fig3, ax3 = plt.subplots(figsize=(12, 8))
    colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(top_majors)))
    bars = ax3.barh(top_majors['major'], top_majors['recommendation_score'], color=colors, edgecolor='black')
    
    ax3.set_xlabel('Recommendation Score', fontsize=12)
    ax3.set_title('Top 15 Majors - Social Media Recommendation Score\n(Based on BERT Sentiment Analysis - Comments + Contents)',
                 fontsize=14, fontweight='bold')
    ax3.invert_yaxis()
    ax3.grid(axis='x', alpha=0.3)
    
    for i, (idx, row) in enumerate(top_majors.iterrows()):
        ax3.text(row['recommendation_score'], i, f" {row['recommendation_score']:.1f}",
                va='center', fontsize=10, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('./output/figures/03_recommendation_ranking.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… Figure 3: Recommendation Ranking")
    
    # å›¾4: æƒ…æ„ŸæŒ‡æ•°å¯¹æ¯”
    top15 = df_sentiment.head(15)
    
    fig4, ax4 = plt.subplots(figsize=(14, 8))
    
    x = np.arange(len(top15))
    width = 0.35
    
    ax4.bar(x - width/2, top15['positive_rate'], width, label='Positive %', 
           color='#2ecc71', alpha=0.8)
    ax4.bar(x + width/2, top15['negative_rate'], width, label='Negative %',
           color='#e74c3c', alpha=0.8)
    
    ax4.set_ylabel('Percentage', fontsize=12)
    ax4.set_title('Top 15 Majors - Positive vs Negative Sentiment Rate',
                 fontsize=14, fontweight='bold')
    ax4.set_xticks(x)
    ax4.set_xticklabels(top15['major'], rotation=45, ha='right')
    ax4.legend()
    ax4.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('./output/figures/04_sentiment_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… Figure 4: Sentiment Comparison")
    
    # å›¾5: èˆ†æƒ… vs å°±ä¸šï¼ˆå¦‚æœæœ‰å°±ä¸šæ•°æ®ï¼‰
    if df_integrated is not None and len(df_integrated) > 0:
        employment_col = 'æœ¬ç§‘å°±ä¸šç‡' if 'æœ¬ç§‘å°±ä¸šç‡' in df_integrated.columns else 'employment_rate'
        
        if employment_col in df_integrated.columns:
            fig5 = px.scatter(
                df_integrated,
                x=employment_col,
                y='sentiment_index',
                size='mention_count',
                color='positive_rate',
                hover_data=['major'],
                text='major',
                color_continuous_scale='RdYlGn',
                title='<b>Social Media Sentiment vs Official Employment Rate</b><br>' +
                      '<sub>Size = Discussion Volume | Color = Positive Sentiment Rate | Data = Comments + Contents</sub>',
                labels={
                    employment_col: 'Official Employment Rate (%)',
                    'sentiment_index': 'Social Media Sentiment Index',
                    'positive_rate': 'Positive %'
                }
            )
            
            fig5.update_traces(textposition='top center', textfont_size=8)
            fig5.update_layout(width=1400, height=800)
            
            fig5.write_html('./output/figures/05_sentiment_vs_employment.html')
            print("âœ… Figure 5: Sentiment vs Employment (Interactive)")
    
    print("\n" + "="*70)
    print("âœ… All basic visualizations completed!")
    print("="*70 + "\n")


def create_content_vs_comment_comparison(df_by_type):
    """åˆ›å»ºè¯„è®º vs å†…å®¹å¯¹æ¯”å›¾"""
    
    print("ğŸ“Š Creating Content vs Comment Comparison...")
    
    # é€è§†è¡¨
    pivot = df_by_type.pivot_table(
        index='major',
        columns='data_type',
        values='positive_rate',
        aggfunc='first'
    ).reset_index()
    
    # é€‰æ‹©åŒæ—¶æœ‰è¯„è®ºå’Œå†…å®¹æ•°æ®çš„ä¸“ä¸š
    valid_cols = [col for col in ['comment', 'content', 'video'] if col in pivot.columns]
    if len(valid_cols) < 2:
        print("  âš ï¸ Not enough data types for comparison")
        return
    
    pivot = pivot.dropna(subset=valid_cols[:2])
    
    if len(pivot) < 5:
        print("  âš ï¸ Not enough majors for comparison")
        return
    
    fig, ax = plt.subplots(figsize=(12, 8))
    
    x = np.arange(len(pivot))
    width = 0.25
    
    colors = ['#3498db', '#e74c3c', '#2ecc71']
    
    for i, col in enumerate(valid_cols):
        if col in pivot.columns:
            ax.bar(x + i*width, pivot[col], width, label=col.title(), 
                  color=colors[i], alpha=0.8, edgecolor='black')
    
    ax.set_ylabel('Positive Sentiment Rate (%)', fontsize=12)
    ax.set_title('Sentiment Comparison: Comments vs Contents vs Videos\n(by Major)',
                fontsize=14, fontweight='bold')
    ax.set_xticks(x + width)
    ax.set_xticklabels(pivot['major'], rotation=45, ha='right')
    ax.legend()
    ax.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('./output/figures/08_content_vs_comment.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ… Figure 8: Content vs Comment Comparison")


# ==================== PART 6: å¯¼å‡ºæ•°æ®è¡¨ ====================

def export_all_tables(df_all, df_sentiment, df_integrated, df_by_type):
    """å¯¼å‡ºæ‰€æœ‰æ•°æ®è¡¨"""
    
    print("ğŸ’¾ Exporting data tables...\n")
    
    # Table 1: åŸå§‹æ•°æ®ï¼ˆå¸¦BERTç»“æœï¼‰
    df_all.to_csv('./output/tables/01_bert_analyzed_all_data.csv', 
                  index=False, encoding='utf-8-sig')
    print("âœ… Table 1: BERT Analyzed All Data (Comments + Contents)")
    
    # Table 2: ä¸“ä¸šèˆ†æƒ…æ±‡æ€»
    df_sentiment.to_csv('./output/tables/02_major_sentiment_summary.csv',
                       index=False, encoding='utf-8-sig')
    print("âœ… Table 2: Major Sentiment Summary")
    
    # Table 3: æŒ‰æ•°æ®ç±»å‹åˆ†ç»„çš„èˆ†æƒ…
    df_by_type.to_csv('./output/tables/03_sentiment_by_major_and_type.csv',
                     index=False, encoding='utf-8-sig')
    print("âœ… Table 3: Sentiment by Major and Data Type")
    
    # Table 4: æ•´åˆæ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
    if df_integrated is not None and len(df_integrated) > 0:
        df_integrated.to_csv('./output/tables/04_integrated_sentiment_employment.csv',
                           index=False, encoding='utf-8-sig')
        print("âœ… Table 4: Integrated Analysis")
    
    print()


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»æ‰§è¡Œæµç¨‹"""
    
    # Step 1: åŠ è½½æ‰€æœ‰æ•°æ®
    loader = RealDataLoader()
    
    # åŠ è½½è¯„è®º
    loader.load_all_comments()
    df_comments = loader.standardize_comments()
    
    # åŠ è½½å†…å®¹/å¸–å­
    loader.load_all_contents()
    df_contents = loader.standardize_contents()
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    df_all = loader.merge_all_data(df_comments, df_contents)
    
    # Step 2: BERTæƒ…æ„Ÿåˆ†æ
    analyzer = RealBERTAnalyzer()
    sentiment_results = analyzer.batch_predict(df_all['text'].tolist())
    
    df_all['sentiment'] = [r[0] for r in sentiment_results]
    df_all['confidence'] = [r[1] for r in sentiment_results]
    
    print("ğŸ“Š BERT Sentiment Analysis Results:")
    print(df_all['sentiment'].value_counts())
    print(f"\nAverage Confidence: {df_all['confidence'].mean():.3f}")
    print(f"\nBy Data Type:")
    print(df_all.groupby('data_type')['sentiment'].value_counts())
    print()
    
    # Step 3: æå–ä¸“ä¸šæåŠ
    df_with_majors = extract_majors_from_text(df_all)
    
    # Step 4: æŒ‰ä¸“ä¸šèšåˆï¼ˆæ€»ä½“ï¼‰
    df_sentiment = aggregate_sentiment_by_major(df_with_majors)
    
    # Step 4b: æŒ‰ä¸“ä¸šå’Œæ•°æ®ç±»å‹èšåˆï¼ˆç”¨äºå¯¹æ¯”ï¼‰
    df_by_type = aggregate_sentiment_by_major_and_type(df_with_majors)
    
    print("ğŸ“Š Top 10 Recommended Majors (by sentiment):")
    print(df_sentiment[['major', 'recommendation_score', 'positive_rate', 'mention_count']].head(10))
    print()
    
    # Step 5: åŠ è½½å°±ä¸šæ•°æ®
    df_employment = load_employment_data()
    
    # Step 6: æ•´åˆæ•°æ®
    df_integrated = None
    if df_employment is not None:
        df_integrated = integrate_sentiment_and_employment(df_sentiment, df_employment)
    
    # Step 7: ç”ŸæˆåŸºç¡€å¯è§†åŒ–
    create_all_visualizations(df_all, df_sentiment, df_integrated)
    
    # Step 8: ç”Ÿæˆå¢å¼ºå¯è§†åŒ–
    create_enhanced_visualizations(df_integrated)
    
    # Step 8b: ç”Ÿæˆå†…å®¹ vs è¯„è®ºå¯¹æ¯”å›¾
    create_content_vs_comment_comparison(df_by_type)
    
    # Step 9: å¯¼å‡ºæ•°æ®è¡¨
    export_all_tables(df_all, df_sentiment, df_integrated, df_by_type)
    
    # æœ€ç»ˆæ€»ç»“
    print("="*70)
    print("âœ… ANALYSIS COMPLETED SUCCESSFULLY!")
    print("="*70)
    print(f"ğŸ“Š Total records analyzed: {len(df_all):,}")
    print(f"   - Comments: {len(df_all[df_all['data_type']=='comment']):,}")
    print(f"   - Contents: {len(df_all[df_all['data_type']=='content']):,}")
    print(f"   - Videos: {len(df_all[df_all['data_type']=='video']):,}")
    print(f"ğŸ¤– BERT model used: uer/roberta-base-finetuned-jd-binary-chinese")
    print(f"ğŸ“ˆ Majors extracted: {len(df_sentiment)}")
    print(f"ğŸ“ Output directory: ./output/")
    print("="*70 + "\n")
    
    return df_all, df_sentiment, df_integrated, df_by_type


# ==================== æ‰§è¡Œ ====================

if __name__ == "__main__":
    df_all, df_sentiment, df_integrated, df_by_type = main()

